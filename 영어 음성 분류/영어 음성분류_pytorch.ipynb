{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import librosa\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wav 데이터 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "africa_train_paths = glob(\"./train/africa/*.wav\")\n",
    "australia_train_paths = glob(\"./train/australia/*.wav\")\n",
    "canada_train_paths = glob(\"./train/canada/*.wav\")\n",
    "england_train_paths = glob(\"./train/england/*.wav\")\n",
    "hongkong_train_paths = glob(\"./train/hongkong/*.wav\")\n",
    "us_train_paths = glob(\"./train/us/*.wav\")\n",
    "\n",
    "path_list = [africa_train_paths, australia_train_paths, canada_train_paths,\n",
    "             england_train_paths, hongkong_train_paths, us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(data):\n",
    "    return np.int(data.split(\"/\")[2].split(\".\")[0])\n",
    "\n",
    "\n",
    "test_ = pd.DataFrame(index=range(0, 6100), columns=[\"path\", \"id\"])\n",
    "test_[\"path\"] = glob(\"./test/*.wav\")\n",
    "test_[\"id\"] = test_[\"path\"].apply(lambda x: get_id(x))\n",
    "\n",
    "test_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "\n",
    "    result = []\n",
    "    for path in tqdm(paths):\n",
    "        # sr = 16000이 의미하는 것은 1초당 16000개의 데이터를 샘플링 한다는 것입니다.\n",
    "        data, sr = librosa.load(path, sr=16000)\n",
    "        result.append(data)\n",
    "    result = np.array(result)\n",
    "    # 메모리가 부족할 때는 데이터 타입을 변경해 주세요 ex) np.array(data, dtype = np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 로드하기 위해서는 많은 시간이 소모 됩니다.\n",
    "# 따라서 추출된 정보를 npy파일로 저장하여 필요 할 때마다 불러올 수 있게 준비합니다.\n",
    "\n",
    "\n",
    "africa_train_data = load_data(africa_train_paths)\n",
    "np.save(\"./npy_data/africa_npy\", africa_train_data)\n",
    "\n",
    "australia_train_data = load_data(australia_train_paths)\n",
    "np.save(\"./npy_data/australia_npy\", australia_train_data)\n",
    "\n",
    "canada_train_data = load_data(canada_train_paths)\n",
    "np.save(\"./npy_data/canada_npy\", canada_train_data)\n",
    "\n",
    "england_train_data = load_data(england_train_paths)\n",
    "np.save(\"./npy_data/england_npy\", england_train_data)\n",
    "\n",
    "hongkong_train_data = load_data(hongkong_train_paths)\n",
    "np.save(\"./npy_data/hongkong_npy\", hongkong_train_data)\n",
    "\n",
    "us_train_data = load_data(us_train_paths)\n",
    "np.save(\"./npy_data/us_npy\", us_train_data)\n",
    "\n",
    "test_data = load_data(test_[\"path\"])\n",
    "np.save(\"./npy_data/test_npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data(test_[\"path\"])\n",
    "np.save(\"./npy_data/test_npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy파일로 저장된 데이터를 불러옵니다.\n",
    "africa_train_data = np.load(\"./npy_data/africa_npy.npy\", allow_pickle=True)\n",
    "australia_train_data = np.load(\n",
    "    \"./npy_data/australia_npy.npy\", allow_pickle=True)\n",
    "canada_train_data = np.load(\"./npy_data/canada_npy.npy\", allow_pickle=True)\n",
    "england_train_data = np.load(\"./npy_data/england_npy.npy\", allow_pickle=True)\n",
    "hongkong_train_data = np.load(\"./npy_data/hongkong_npy.npy\", allow_pickle=True)\n",
    "us_train_data = np.load(\"./npy_data/us_npy.npy\", allow_pickle=True)\n",
    "\n",
    "\n",
    "train_data_list = [africa_train_data, australia_train_data[:1000], canada_train_data[:1000],\n",
    "                   england_train_data[:2500], hongkong_train_data[:1000], us_train_data[:2500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(\"./npy_data/test_npy.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 대회에서 음성은 각각 다른 길이를 갖고 있습니다.\n",
    "# baseline 코드에서는 음성 중 길이가 가장 작은 길이의 데이터를 기준으로 데이터를 잘라서 사용합니다.\n",
    "\n",
    "def get_mini(data):\n",
    "\n",
    "    mini = 9999999\n",
    "    for i in data:\n",
    "        if len(i) < mini:\n",
    "            mini = len(i)\n",
    "\n",
    "    return mini\n",
    "\n",
    "# 음성들의 길이를 맞춰줍니다.\n",
    "\n",
    "\n",
    "def set_length(data, d_mini):\n",
    "\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.append(i[:d_mini])\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "# feature를 생성합니다.\n",
    "\n",
    "\n",
    "def get_feature(data, sr=16000, n_fft=256, win_length=200, hop_length=160, n_mels=64):\n",
    "    mel = []\n",
    "    for i in data:\n",
    "        # win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n",
    "        # hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n",
    "        # n_mels 는 적용할 mel filter의 개수입니다.\n",
    "        mel_ = librosa.feature.melspectrogram(\n",
    "            i, sr=sr, n_fft=n_fft, win_length=win_length, hop_length=hop_length, n_mels=n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    mel_mean = mel.mean()\n",
    "    mel_std = mel.std()\n",
    "    mel = (mel - mel_mean) / mel_std\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.concatenate(train_data_list, axis=0)\n",
    "test_x = np.array(test_data)\n",
    "\n",
    "# 음성의 길이 중 가장 작은 길이를 구합니다.\n",
    "\n",
    "train_mini = get_mini(train_x)\n",
    "test_mini = get_mini(test_x)\n",
    "\n",
    "mini = np.min([train_mini, test_mini])\n",
    "\n",
    "# data의 길이를 가장 작은 길이에 맞춰 잘라줍니다.\n",
    "\n",
    "train_x = set_length(train_x, mini)\n",
    "test_x = set_length(test_x, mini)\n",
    "\n",
    "# librosa를 이용해 feature를 추출합니다.\n",
    "\n",
    "train_x = get_feature(data=train_x)\n",
    "test_x = get_feature(data=test_x)\n",
    "\n",
    "train_x = train_x.reshape(-1, train_x.shape[1], train_x.shape[2], 1)\n",
    "test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data의 label을 생성해 줍니다.\n",
    "\n",
    "train_y = np.concatenate((np.zeros(2500, dtype=np.int),\n",
    "                          np.ones(1000, dtype=np.int),\n",
    "                          np.ones(1000, dtype=np.int) * 2,\n",
    "                          np.ones(2500, dtype=np.int) * 3,\n",
    "                          np.ones(1000, dtype=np.int) * 4,\n",
    "                          np.ones(2500, dtype=np.int) * 5), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample['x'], sample['y']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        x = x.transpose((2, 0, 1))\n",
    "        return {'x': torch.FloatTensor(x),\n",
    "                'y': torch.FloatTensor(y)}\n",
    "\n",
    "\n",
    "to_tensor = transforms.Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_x, train_y, transforms=to_tensor):\n",
    "        self.x_data = train_x\n",
    "        self.y_data = train_y\n",
    "        self.transforms = transforms  # Transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        sample = {'x': x, 'y': y}\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        y = y.astype(np.float32)\n",
    "\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data(X, y):\n",
    "    \"\"\"\n",
    "    train 데이터를 8:2 비율로 train과 valid 데이터로 나누는 함수\n",
    "\n",
    "    \"\"\"\n",
    "    X_len = len(X)\n",
    "    idx = list(range(X_len))\n",
    "\n",
    "    train_idx, valid_idx, _, _ = train_test_split(\n",
    "        idx, y, stratify=y, test_size=0.2)\n",
    "\n",
    "    train_x, train_y = X[train_idx], y[train_idx]\n",
    "    valid_x, valid_y = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "def get_dataloader(X, y, mode, batch_size):\n",
    "    \"\"\"\n",
    "    데이터프레임을 dataloader형태로 반환하는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    if mode == 'TRAIN':\n",
    "        train_x, train_y, valid_x, valid_y = get_split_data(X, y)\n",
    "\n",
    "        # Data Loader\n",
    "        train_dataset = CustomDataset(train_x, train_y)\n",
    "        valid_dataset = CustomDataset(valid_x, valid_y)\n",
    "\n",
    "        train_data_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=3\n",
    "        )\n",
    "        valid_data_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=3\n",
    "        )\n",
    "        return train_data_loader, valid_data_loader\n",
    "\n",
    "    else:\n",
    "        test_dataset = CustomDataset(X, y)\n",
    "        test_data_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=3,\n",
    "            drop_last=False\n",
    "        )\n",
    "        return test_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiLabeleffnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabeleffnet, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(64, 3, 1, stride=1)\n",
    "        self.resnet = models.efficientnet_b7()\n",
    "        self.FC = nn.Linear(1000, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # resnet의 입력은 [3, N, N]으로\n",
    "        # 3개의 채널을 갖기 때문에\n",
    "        # resnet 입력 전에 conv2d를 한 층 추가\n",
    "        x = F.silu(self.conv2d(x))\n",
    "\n",
    "        # resnet18을 추가\n",
    "        x = F.silu(self.resnet(x))\n",
    "\n",
    "        # 마지막 출력에 nn.Linear를 추가\n",
    "        # multilabel을 예측해야 하기 때문에\n",
    "        # softmax가 아닌 sigmoid를 적용\n",
    "        #x = self.FC(x)\n",
    "        x = self.FC(x)\n",
    "        return x\n",
    "# 모델 선언\n",
    "\n",
    "#model = MultiLabeleffnet()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    train 데이터로 모델을 학습하고 valid 데이터로 모델을 검증하는 코드\n",
    "\n",
    "    파라미터\n",
    "    ---\n",
    "    model : \n",
    "        학습할 모델\n",
    "    dataloaders_dict : dict\n",
    "        train_dataloader과 validation_datalodaer가 들어 있는 dictonary\n",
    "    optimizer : \n",
    "        최적화 함수\n",
    "    num_epochs : int\n",
    "        학습 횟수\n",
    "    device : cuda or cpu\n",
    "        모델을 학습할 때 사용할 장비\n",
    "\n",
    "    returns \n",
    "    best_model :\n",
    "        검증데이터 셋 기준으로 가장 성능이 좋은 모델\n",
    "    ---\n",
    "    \"\"\"\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=5,\n",
    "                                                   gamma=0.9)\n",
    "    model.to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "    critrion = torch.nn.CrossEntropyLoss()\n",
    "    # 학습이 어느정도 진행되면 gpu 가속화\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # loss가 제일 낮은 모델을 찾기위한 변수\n",
    "    best_val_loss = int(1e9)\n",
    "    best_model = model\n",
    "    for epoch in range(num_epochs):\n",
    "        # epoch 별 학습 및 검증\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 학습 모드로\n",
    "            else:\n",
    "                model.eval()   # 모델을 추론 모드로\n",
    "\n",
    "            epoch_loss = 0.0  # epoch loss\n",
    "            epoch_corrects = 0  # epoch 정확도\n",
    "            for i,  batch in enumerate(dataloaders_dict[phase]):\n",
    "                images, labels = batch\n",
    "                # tensor를 gpu에 올리기\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 옵티마이저 초기화 초기화\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 순전파 계산\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    probs = model(images)\n",
    "\n",
    "                    loss = critrion(probs, labels.long())\n",
    "\n",
    "                    # 학습시 역전파\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # 결과 계산\n",
    "                    # loss계산\n",
    "                    epoch_loss += loss.item()\n",
    "                    # 정확도 계산\n",
    "                    # train accuracy 계산\n",
    "                    probs = probs.cpu().detach().numpy()\n",
    "                    labels = labels.cpu().detach().numpy()\n",
    "                    batch_acc = (labels == np.argmax(probs, axis=1)).mean()\n",
    "                    epoch_corrects += batch_acc\n",
    "            # epoch별 loss 및 정확도\n",
    "\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase])\n",
    "            epoch_acc = 100 * epoch_corrects / \\\n",
    "                len(dataloaders_dict[phase])\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "            if phase == 'valid' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model = model\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_models = []\n",
    "# 교차 검증을 진행할 K\n",
    "K = 1\n",
    "for i in range(K):\n",
    "\n",
    "    train_loader, val_loader = get_dataloader(\n",
    "        train_x, train_y, mode='TRAIN', batch_size=256)\n",
    "    #  dict 형식으로 data loader 정의\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "    num_epochs = 100\n",
    "    max_grad_norm = 1\n",
    "    learning_rate = 5e-5\n",
    "\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('사용하는 device :', device)\n",
    "    print('-----------------', i+1, '/', K,\n",
    "          '- fold start-----------------')\n",
    "\n",
    "    model = MultiLabeleffnet()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_model = train_model(model, dataloaders_dict,\n",
    "                             optimizer, num_epochs, device)\n",
    "    best_models.append(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    pred_ = []\n",
    "    for idx, sample in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            # 추론\n",
    "            model.eval()\n",
    "            images, _ = sample\n",
    "            images = images.to(device)\n",
    "            probs = model(images)\n",
    "            probs = F.softmax(probs)\n",
    "            probs = probs.cpu().detach().numpy()\n",
    "            pred_.append(probs)\n",
    "    return pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold 별 best model을 이용하여 inference 진행\n",
    "# 이후 결과를 hard voting 진행\n",
    "preds = []\n",
    "for best_model in best_models:\n",
    "\n",
    "    test_y = np.array(range(len(test_x)))\n",
    "    test_loader = get_dataloader(\n",
    "        test_x, test_y, mode='TEST', batch_size=256)\n",
    "    preds.append(inference(test_loader, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = np.argmax(preds,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_type(data):\n",
    "    return np.int(data)\n",
    "\n",
    "\n",
    "# 처음에 살펴본 것처럼 glob로 test data의 path는 sample_submission의 id와 같이 1,2,3,4,5.....으로 정렬 되어있지 않습니다.\n",
    "# 만들어둔 test_ 데이터프레임을 이용하여 sample_submission과 predict값의 id를 맞춰줍니다.\n",
    "sample_submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "result = pd.concat([test_, pd.DataFrame(\n",
    "    np.mean(final_pred, axis=0))], axis=1).iloc[:, 1:]\n",
    "result[\"id\"] = result[\"id\"].apply(lambda x: cov_type(x))\n",
    "\n",
    "result = pd.merge(sample_submission[\"id\"], result)\n",
    "result.columns = sample_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
