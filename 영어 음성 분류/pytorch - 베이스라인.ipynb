{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 베이스라인을 바탕으로 pytorch 코드를 작성해 보았습니다. \n",
    "- 교차검증, datat augment 등 추가할 부분이 많은 코드입니다.\n",
    "- 코드에 문제가 있거나 의문이 있으시면 댓글 남겨주세요 !! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import librosa\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "africa_train_paths = glob(\"./open/train/africa/*.wav\")\n",
    "australia_train_paths = glob(\"./open/train/australia/*.wav\")\n",
    "canada_train_paths = glob(\"./open/train/canada/*.wav\")\n",
    "england_train_paths = glob(\"./open/train/england/*.wav\")\n",
    "hongkong_train_paths = glob(\"./open/train/hongkong/*.wav\")\n",
    "us_train_paths = glob(\"./open/train/us/*.wav\")\n",
    "\n",
    "path_list = [africa_train_paths, australia_train_paths, canada_train_paths,\n",
    "             england_train_paths, hongkong_train_paths, us_train_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob로 test data의 path를 불러올때 순서대로 로드되지 않을 경우를 주의해야 합니다.\n",
    "# test_ 데이터 프레임을 만들어서 나중에 sample_submission과 id를 기준으로 merge시킬 준비를 합니다.\n",
    "\n",
    "def get_id(data):\n",
    "    return np.int(data.split(\"\\\\\")[1].split(\".\")[0])\n",
    "\n",
    "test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n",
    "test_[\"path\"] = glob(\"./open/test/*.wav\")\n",
    "test_[\"id\"] = test_[\"path\"].apply(lambda x : get_id(x))\n",
    "\n",
    "test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "\n",
    "    result = []\n",
    "    for path in tqdm(paths):\n",
    "        # sr = 16000이 의미하는 것은 1초당 16000개의 데이터를 샘플링 한다는 것입니다.\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        result.append(data)\n",
    "    result = np.array(result) \n",
    "    # 메모리가 부족할 때는 데이터 타입을 변경해 주세요 ex) np.array(data, dtype = np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 로드하기 위해서는 많은 시간이 소모 됩니다.\n",
    "# 따라서 추출된 정보를 npy파일로 저장하여 필요 할 때마다 불러올 수 있게 준비합니다.\n",
    "\n",
    "os.mkdir(\"./npy_data\")\n",
    "\n",
    "africa_train_data = load_data(africa_train_paths)\n",
    "np.save(\"./npy_data/africa_npy\", africa_train_data)\n",
    "\n",
    "australia_train_data = load_data(australia_train_paths)\n",
    "np.save(\"./npy_data/australia_npy\", australia_train_data)\n",
    "\n",
    "canada_train_data = load_data(canada_train_paths)\n",
    "np.save(\"./npy_data/canada_npy\", canada_train_data)\n",
    "\n",
    "england_train_data = load_data(england_train_paths)\n",
    "np.save(\"./npy_data/england_npy\", england_train_data)\n",
    "\n",
    "hongkong_train_data = load_data(hongkong_train_paths)\n",
    "np.save(\"./npy_data/hongkong_npy\", hongkong_train_data)\n",
    "\n",
    "us_train_data = load_data(us_train_paths)\n",
    "np.save(\"./npy_data/us_npy\", us_train_data)\n",
    "\n",
    "test_data = load_data(test_[\"path\"])\n",
    "np.save(\"./npy_data/test_npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy파일로 저장된 데이터를 불러옵니다.\n",
    "africa_train_data = np.load(\"./npy_data/africa_npy.npy\", allow_pickle = True)\n",
    "australia_train_data = np.load(\"./npy_data/australia_npy.npy\", allow_pickle = True)\n",
    "canada_train_data = np.load(\"./npy_data/canada_npy.npy\", allow_pickle = True)\n",
    "england_train_data = np.load(\"./npy_data/england_npy.npy\", allow_pickle = True)\n",
    "hongkong_train_data = np.load(\"./npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
    "us_train_data = np.load(\"./npy_data/us_npy.npy\", allow_pickle = True)\n",
    "\n",
    "test_data = np.load(\"./npy_data/test_npy.npy\", allow_pickle = True)\n",
    "\n",
    "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 대회에서 음성은 각각 다른 길이를 갖고 있습니다.\n",
    "# baseline 코드에서는 음성 중 길이가 가장 작은 길이의 데이터를 기준으로 데이터를 잘라서 사용합니다.\n",
    "\n",
    "def get_mini(data):\n",
    "\n",
    "    mini = 9999999\n",
    "    for i in data:\n",
    "        if len(i) < mini:\n",
    "            mini = len(i)\n",
    "\n",
    "    return mini\n",
    "\n",
    "#음성들의 길이를 맞춰줍니다.\n",
    "\n",
    "def set_length(data, d_mini):\n",
    "\n",
    "    result = []\n",
    "    for i in data:\n",
    "        result.append(i[:d_mini])\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "#feature를 생성합니다.\n",
    "\n",
    "def get_feature(data, sr = 16000, n_fft = 256, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in data:\n",
    "        # win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n",
    "        # hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n",
    "        # n_mels 는 적용할 mel filter의 개수입니다.\n",
    "        mel_ = librosa.feature.melspectrogram(i, sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "\n",
    "    mel_mean = mel.mean()\n",
    "    mel_std = mel.std()\n",
    "    mel = (mel - mel_mean) / mel_std\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.concatenate(train_data_list, axis= 0)\n",
    "test_x = np.array(test_data)\n",
    "\n",
    "# 음성의 길이 중 가장 작은 길이를 구합니다.\n",
    "\n",
    "train_mini = get_mini(train_x)\n",
    "test_mini = get_mini(test_x)\n",
    "\n",
    "mini = np.min([train_mini, test_mini])\n",
    "\n",
    "# data의 길이를 가장 작은 길이에 맞춰 잘라줍니다.\n",
    "\n",
    "train_x = set_length(train_x, mini)\n",
    "test_x = set_length(test_x, mini)\n",
    "\n",
    "# librosa를 이용해 feature를 추출합니다.\n",
    "\n",
    "train_x = get_feature(data = train_x)\n",
    "test_x = get_feature(data = test_x)\n",
    "\n",
    "train_x = train_x.reshape(-1, train_x.shape[1], train_x.shape[2], 1)\n",
    "test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.concatenate((np.zeros(len(africa_train_data), dtype = np.int),\n",
    "                        np.ones(len(australia_train_data), dtype = np.int),\n",
    "                         np.ones(len(canada_train_data), dtype = np.int) * 2,\n",
    "                         np.ones(len(england_train_data), dtype = np.int) * 3,\n",
    "                         np.ones(len(hongkong_train_data), dtype = np.int) * 4,\n",
    "                         np.ones(len(us_train_data), dtype = np.int) * 5), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.get_dummies(train_y).to_numpy(dtype = 'long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample['x'], sample['y']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        x = x.transpose((2, 0, 1))\n",
    "        return {'x': torch.FloatTensor(x),\n",
    "                'y': torch.FloatTensor(y)}\n",
    "to_tensor = transforms.Compose([\n",
    "                      ToTensor() \n",
    "])\n",
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self,train_x,train_y,transforms=to_tensor):\n",
    "        self.x_data = train_x\n",
    "        self.y_data = train_y\n",
    "        self.transforms = transforms# Transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        \n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        sample = {'x': x, 'y': y}\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        y = y.astype(np.float32)\n",
    "        \n",
    "        \n",
    "        return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelResnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelResnet, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(64, 3, 1, stride=1)\n",
    "        self.resnet = timm.create_model('resnet101', pretrained=False) \n",
    "        self.FC = nn.Linear(1000, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # resnet의 입력은 [3, N, N]으로\n",
    "        # 3개의 채널을 갖기 때문에\n",
    "        # resnet 입력 전에 conv2d를 한 층 추가\n",
    "        x = F.relu(self.conv2d(x))\n",
    "\n",
    "        # resnet18을 추가\n",
    "        x = F.relu(self.resnet(x))\n",
    "\n",
    "        # 마지막 출력에 nn.Linear를 추가\n",
    "        # multilabel을 예측해야 하기 때문에\n",
    "        # softmax가 아닌 sigmoid를 적용\n",
    "        #x = self.FC(x)\n",
    "        x = torch.sigmoid(self.FC(x))\n",
    "        return x\n",
    "# 모델 선언\n",
    "\n",
    "model = MultiLabelResnet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_x,train_y)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion  = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 모델의 dropoupt, batchnormalization를 train 모드로 설정\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    # 1개 epoch 훈련\n",
    "    train_acc_list = []\n",
    "    with tqdm(dataloader,#train_data_loader를 iterative하게 반환\n",
    "            total=dataloader.__len__(), # train_data_loader의 크기\n",
    "            unit=\"batch\") as train_bar: # 한번 반환하는 smaple의 단위는 \"batch\"\n",
    "        for idx,sample in enumerate(train_bar):\n",
    "            if idx == 224 :\n",
    "                break\n",
    "            train_bar.set_description(f\"Train Epoch {epoch}\")\n",
    "            # 갱신할 변수들에 대한 모든 변화도를 0으로 초기화\n",
    "            # 참고)https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images, labels = sample\n",
    "            # tensor를 gpu에 올리기 \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            \n",
    "            # .forward()에서 중간 노드의 gradient를 계산\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # 모델 예측\n",
    "                probs = model(images)\n",
    "                probs = F.softmax(probs)\n",
    "               # probs = (probs == probs.max()) * 1.0\n",
    "                #loss = criterion(probs, y_train)\n",
    "                \n",
    "                \n",
    "                loss = criterion(probs, labels)\n",
    "                #loss = criterion(probs, torch.max(y_train, 1)[1])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                probs  = probs.cpu().detach().numpy()\n",
    "                labels = labels.cpu().detach().numpy()\n",
    "                # train accuracy 계산\n",
    "                cnt = 0\n",
    "                for i in range(256):\n",
    "                    \n",
    "                    if probs[i].argmax() == labels[i].argmax():\n",
    "                        cnt +=1\n",
    "                \n",
    "                #preds = probs > 0.5\n",
    "                #batch_acc = (labels == preds).mean()\n",
    "                batch_acc = cnt/256\n",
    "                train_acc_list.append(batch_acc)\n",
    "                train_acc = np.mean(train_acc_list)\n",
    "\n",
    "            # 현재 progress bar에 현재 미니배치의 loss 결과 출력\n",
    "            train_bar.set_postfix(train_loss= loss.item(),\n",
    "                                    train_acc = train_acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Epoch 0:  90%|████████▉ | 224/250 [01:35<00:11,  2.34batch/s, train_acc=0.304, train_loss=0.446]  \n",
    "Train Epoch 1:  90%|████████▉ | 224/250 [01:35<00:11,  2.34batch/s, train_acc=0.393, train_loss=0.443]  \n",
    "Train Epoch 2:  90%|████████▉ | 224/250 [01:35<00:11,  2.34batch/s, train_acc=0.395, train_loss=0.441]  \n",
    "Train Epoch 3:  90%|████████▉ | 224/250 [01:35<00:11,  2.34batch/s, train_acc=0.396, train_loss=0.436]  \n",
    "Train Epoch 4:  90%|████████▉ | 224/250 [01:35<00:11,  2.34batch/s, train_acc=0.395, train_loss=0.436]  \n",
    "Train Epoch 5:  90%|████████▉ | 224/250 [01:35<00:11,  2.33batch/s, train_acc=0.395, train_loss=0.432]  \n",
    "Train Epoch 6:  90%|████████▉ | 224/250 [01:36<00:11,  2.33batch/s, train_acc=0.396, train_loss=0.43]   \n",
    "Train Epoch 7:  90%|████████▉ | 224/250 [01:36<00:11,  2.33batch/s, train_acc=0.396, train_loss=0.429]  \n",
    "Train Epoch 8:  90%|████████▉ | 224/250 [01:35<00:11,  2.34batch/s, train_acc=0.395, train_loss=0.423]  \n",
    "Train Epoch 9:  90%|████████▉ | 224/250 [01:36<00:11,  2.33batch/s, train_acc=0.397, train_loss=0.424]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = pd.DataFrame(index=range(0,len(test_x)), columns=['0', '1', '2', '3', '4', '5'])\n",
    "test_y = test_y.fillna(0).to_numpy()\n",
    "dataset = CustomDataset(test_x,test_y)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ = []\n",
    "for idx, sample in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "        # 추론\n",
    "        model.eval()\n",
    "        images,_ = sample\n",
    "        images = images.to(device)\n",
    "        probs  = model(images)\n",
    "        probs = F.softmax(probs)\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        pred_.append(probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_type(data):\n",
    "    return np.int(data)\n",
    "\n",
    "# 처음에 살펴본 것처럼 glob로 test data의 path는 sample_submission의 id와 같이 1,2,3,4,5.....으로 정렬 되어있지 않습니다.\n",
    "# 만들어둔 test_ 데이터프레임을 이용하여 sample_submission과 predict값의 id를 맞춰줍니다.\n",
    "sample_submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "result = pd.concat([test_, pd.DataFrame(np.mean(pred, axis = 0))], axis = 1).iloc[:, 1:]\n",
    "result[\"id\"] = result[\"id\"].apply(lambda x : cov_type(x))\n",
    "\n",
    "result = pd.merge(sample_submission[\"id\"], result)\n",
    "result.columns = sample_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"DACON.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
