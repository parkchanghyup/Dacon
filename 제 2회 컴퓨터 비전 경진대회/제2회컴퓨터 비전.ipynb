{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 2회 컴퓨터 비전 학습 경진 대회\n",
    "- 대회 기간 : 2021.01.27 ~ 2021.03.01 17:59 \n",
    "- public score : 0.86792\n",
    "- private score : 0.86733\n",
    "- 사용 모델 : efficient net-b7\n",
    "- 대회 task : 이미지 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:44:37.226816Z",
     "start_time": "2022-02-23T13:44:37.209841Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:44:39.698360Z",
     "start_time": "2022-02-23T13:44:37.386719Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시드고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:44:39.713354Z",
     "start_time": "2022-02-23T13:44:39.700356Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:44:43.154232Z",
     "start_time": "2022-02-23T13:44:43.147251Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy를 tensor로 변환하는 ToTensor 정의\n",
    "class ToTensor(object):\n",
    "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.FloatTensor(image),\n",
    "                'label': torch.FloatTensor(label)}\n",
    "\n",
    "\n",
    "class DatasetMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dir_path,\n",
    "                 meta_df,\n",
    "                 augmentations=None):\n",
    "\n",
    "        self.dir_path = dir_path  # 데이터의 이미지가 저장된 디렉터리 경로\n",
    "        self.meta_df = meta_df  # 데이터의 인덱스와 정답지가 들어있는 DataFrame\n",
    "        self.augmentations = augmentations  # Augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 폴더 경로 + 이미지 이름 + .png => 파일의 경로\n",
    "        # 참고) \"12\".zfill(5) => 000012\n",
    "        #       \"146\".zfill(5) => 000145\n",
    "        # cv2.IMREAD_GRAYSCALE : png파일을 채널이 1개인 GRAYSCALE로 읽음\n",
    "        image = cv2.imread(self.dir_path +\n",
    "                           str(self.meta_df.iloc[index, 0]).zfill(5) + '.png',\n",
    "                           cv2.IMREAD_GRAYSCALE)\n",
    "        # 0 ~ 255의 값을 갖고 크기가 (256,256)인 numpy array를\n",
    "        # 0 ~ 1 사이의 실수를 갖고 크기가 (256,256,1)인 numpy array로 변환\n",
    "        image = (image/255).astype('float32')[..., np.newaxis]\n",
    "\n",
    "        # 정답 numpy array생성(존재하면 1 없으면 0)\n",
    "        label = self.meta_df.iloc[index, 1:].values.astype('float')\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        # augmentation 적용\n",
    "        if self.augmentations:\n",
    "            sample['image'] = self.augmentations(sample['image'])\n",
    "        # sample 반환\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:44:44.722615Z",
     "start_time": "2022-02-23T13:44:44.712642Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_split_data(df):\n",
    "    \"\"\"\n",
    "    train 데이터를 8:2 비율로 train과 valid 데이터로 나누는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = df.loc[:,'index':'y'], df['z']\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        X, y,  test_size=0.2)\n",
    "\n",
    "    train_x['z'] = train_y\n",
    "    valid_x['z'] = valid_y\n",
    "\n",
    "    return train_x, valid_x\n",
    "\n",
    "\n",
    "def get_dataloader(dirty_mnist_answer,mode,batch_size):\n",
    "    \"\"\"\n",
    "    데이터프레임을 dataloader형태로 반환하는 함수\n",
    "    \"\"\"\n",
    "    # augmentations 적용 \n",
    "    train_augmentations = transforms.Compose([\n",
    "            transforms.ToPILImage(),        \n",
    "            transforms.RandomHorizontalFlip(p=0.6),\n",
    "            transforms.RandomVerticalFlip(0.6),\n",
    "            transforms.RandomRotation(40),\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "    valid_augmentations = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    if mode == 'TRAIN':\n",
    "        train,valid = get_split_data(dirty_mnist_answer)\n",
    "        \n",
    "        # Data Loader\n",
    "        train_dataset = DatasetMNIST(\"dirty_mnist/\", train, augmentations =train_augmentations)\n",
    "        valid_dataset = DatasetMNIST(\"dirty_mnist/\", valid, augmentations = valid_augmentations)\n",
    "\n",
    "        train_data_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True,\n",
    "            num_workers = 0\n",
    "        )\n",
    "        valid_data_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = 0\n",
    "        )\n",
    "        return train_data_loader, valid_data_loader\n",
    "    \n",
    "    else :\n",
    "        test_dataset = DatasetMNIST(\"test_dirty_mnist/\", dirty_mnist_answer)\n",
    "        test_data_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = 0,\n",
    "            drop_last = False\n",
    "        )\n",
    "        return test_data_loader\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:44:45.159363Z",
     "start_time": "2022-02-23T13:44:45.141410Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    train 데이터로 모델을 학습하고 valid 데이터로 모델을 검증하는 코드\n",
    "\n",
    "    파라미터\n",
    "    ---\n",
    "    model : \n",
    "        학습할 모델\n",
    "    dataloaders_dict : dict\n",
    "        train_dataloader과 validation_datalodaer가 들어 있는 dictonary\n",
    "    optimizer : \n",
    "        최적화 함수\n",
    "    num_epochs : int\n",
    "        학습 횟수\n",
    "    device : cuda or cpu\n",
    "        모델을 학습할 때 사용할 장비\n",
    "\n",
    "    returns \n",
    "    best_model :\n",
    "        검증데이터 셋 기준으로 가장 성능이 좋은 모델\n",
    "    ---\n",
    "    \"\"\"\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=5,\n",
    "                                                   gamma=0.9)\n",
    "    model.to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "    critrion = torch.nn.CrossEntropyLoss()\n",
    "    # 학습이 어느정도 진행되면 gpu 가속화\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # loss가 제일 낮은 모델을 찾기위한 변수\n",
    "    best_val_loss = int(1e9)\n",
    "    for epoch in range(num_epochs):\n",
    "        # epoch 별 학습 및 검증\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 학습 모드로\n",
    "            else:\n",
    "                model.eval()   # 모델을 추론 모드로\n",
    "\n",
    "            epoch_loss = 0.0  # epoch loss\n",
    "            epoch_corrects = 0  # epoch 정확도\n",
    "            for i,  batch in enumerate(dataloaders_dict[phase]):\n",
    "                images, labels = batch['image'], batch['label']\n",
    "                # tensor를 gpu에 올리기\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 옵티마이저 초기화 초기화\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 순전파 계산\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    probs = model(images)\n",
    "                    loss = critrion(probs, labels)\n",
    "\n",
    "                    # 학습시 역전파\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # 결과 계산\n",
    "                    # loss계산\n",
    "                    epoch_loss += loss.item() * len(probs)\n",
    "                    # 정확도 계산\n",
    "                    # train accuracy 계산\n",
    "                    probs = probs.cpu().detach().numpy()\n",
    "                    labels = labels.cpu().detach().numpy()\n",
    "                    preds = probs > 0.5\n",
    "                    batch_acc = (labels == preds).mean()\n",
    "                    epoch_corrects += batch_acc\n",
    "            # epoch별 loss 및 정확도\n",
    "\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase])\n",
    "            epoch_acc = 100 * epoch_corrects / \\\n",
    "                len(dataloaders_dict[phase])\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "            if phase=='val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model = model\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T13:39:16.220133Z",
     "start_time": "2022-02-23T13:39:16.031618Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import effnet_b7\n",
    "from model import regnet\n",
    "from model import mobilenet_v3\n",
    "\n",
    "#model = regnet.MultiLabelRegnet()\n",
    "#model = mobilenet_v3.moblienet_v3()\n",
    "\n",
    "# 제일 성능이 좋은 effnet 사용.\n",
    "model = effnet_b7.MultiLabeleffnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-23T13:44:48.410Z"
    }
   },
   "outputs": [],
   "source": [
    "dirty_mnist_answer = pd.read_csv(\"dirty_mnist_2nd_answer.csv\")\n",
    "# dirty_mnist라는 디렉터리 속에 들어있는 파일들의 이름을\n",
    "# namelist라는 변수에 저장\n",
    "namelist = os.listdir('./dirty_mnist/')\n",
    "\n",
    "best_models = []\n",
    "# 교차 검증을 진행할 K\n",
    "K = 1\n",
    "for i in range(K):\n",
    "\n",
    "    train_loader, val_loader = get_dataloader(\n",
    "        dirty_mnist_answer, mode='TRAIN', batch_size=8)\n",
    "    #  dict 형식으로 data loader 정의\n",
    "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "    num_epochs = 30\n",
    "    max_grad_norm = 1\n",
    "    learning_rate = 5e-5\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('사용하는 device :', device)\n",
    "    print('--------------------', i+1, '/', K,\n",
    "          '- fold start--------------------')\n",
    "\n",
    "    model = MultiLabeleffnet()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_model = train_model(model, dataloaders_dict,\n",
    "                             optimizer, num_epochs, device)\n",
    "    best_models.append(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-23T13:44:49.073Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(best_models, prediction_df, test_dataloader):\n",
    "    \"\"\"\n",
    "    test데이터 셋을 inference 하는 코드\n",
    "\n",
    "    파라미터\n",
    "    ---\n",
    "    best_models : \n",
    "        k개의 fold에서 가장 성능 좋은 모델들\n",
    "    prediction_df : dataframe\n",
    "        예측을 후 결과 값을 채우기 위해 필요한 dataframe\n",
    "    test_dataloader : dataloader\n",
    "        mini batch를 위한 data loader\n",
    "    returns\n",
    "    ---\n",
    "    probs_list : list\n",
    "        test 데이터의 예측값을 가지고 있는 list\n",
    "    \"\"\"\n",
    "    probs_list = []\n",
    "    # 5개의 fold마다 가장 좋은 모델을 이용하여 예측\n",
    "    for model in best_models:\n",
    "        # 0으로 채워진 array 생성\n",
    "        probs_array = np.zeros([prediction_df.shape[0],\n",
    "                                prediction_df.shape[1] - 1])\n",
    "\n",
    "        for idx, batch in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                # 추론\n",
    "                model.eval()\n",
    "                images = batch['image']\n",
    "                images = images.to(device)\n",
    "                probs = model(images)\n",
    "                probs = probs.cpu().detach().numpy()\n",
    "\n",
    "                # 예측 결과를\n",
    "                # prediction_array에 입력\n",
    "                batch_index = batch_size * idx\n",
    "                probs_array[batch_index: batch_index + images.shape[0], :]\\\n",
    "                    = probs\n",
    "        probs_list.append(probs_array[..., np.newaxis])\n",
    "    return probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-23T13:44:49.648Z"
    }
   },
   "outputs": [],
   "source": [
    "# test Dataset 정의\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "batch_size = 128\n",
    "test_dataloader = get_dataloader(\n",
    "    sample_submission, 'TEST', batch_size=batch_size)\n",
    "\n",
    "prediction_df = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list = inference(best_models, prediction_df,test_dataloader)\n",
    "prob_list = np.concatenate(prob_list, axis = 2)\n",
    "probs_mean = prob_list.mean(axis = 2)\n",
    "probs_mean = (probs_mean > 0.5) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정답파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission.iloc[:,1:] = probs_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eunil_py38",
   "language": "python",
   "name": "eunil_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
