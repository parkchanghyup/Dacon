{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from adamp import AdamP\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu111'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45654, 3)\n",
      "(9131, 2)\n"
     ]
    }
   ],
   "source": [
    "# 학습용 데이터셋 불러오기\n",
    "import pandas as pd\n",
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:,1:]\n",
    "test = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분석기(Okt) 불러오기 \n",
    "okt=Okt() \n",
    "\n",
    "# 조사, 어미, 구두점 제거\n",
    "def func(text):\n",
    "    clean = []\n",
    "    for word in okt.pos(text, stem=True): #어간 추출\n",
    "        if word[1] not in ['Josa', 'Eomi', 'Punctuation']: #조사, 어미, 구두점 제외 \n",
    "            clean.append(word[0])\n",
    "    \n",
    "    \n",
    "    return \" \".join(clean) \n",
    "\n",
    "train['title'] = train['title'].apply(lambda x : func(x))\n",
    "test['title'] = test['title'].apply(lambda x : func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: 36523\n",
      "valid shape is: 9131\n"
     ]
    }
   ],
   "source": [
    "# Train / Test set 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(\"train shape is:\", len(train))\n",
    "print(\"valid shape is:\", len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer ,AutoModelForSequenceClassification\n",
    "\n",
    "#model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tok = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-large',num_labels=7).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class robertaadataset(Dataset):  \n",
    "    def __init__(self, dataset,max_len,bert_tokenizer):\n",
    "        \n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.dataset = dataset\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, :2].values\n",
    "        text = row[0]\n",
    "        y = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "    \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = 32\n",
    "num_epochs = 10\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 옵티마이저 선언\n",
    "optimizer = AdamP(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = robertaadataset(train,max_len,tok)\n",
    "test_dataset = robertaadataset(valid,max_len,tok)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## 1 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac319da5482b4bf38f31d5315242ed49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 63.34794284403324 Accuracy: 0.7875\n",
      "Batch Loss: 107.88995760679245 Accuracy: 0.8225\n",
      "Batch Loss: 150.9573018103838 Accuracy: 0.8351041666666666\n",
      "Batch Loss: 191.8354904204607 Accuracy: 0.8417578125\n",
      "Batch Loss: 231.4056955575943 Accuracy: 0.84578125\n",
      "Train Loss: 259.1287747323513 Train Accuracy: 0.8485885606330258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9bad5c10634366911fd7044af80e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8813930566203044\n",
      "\n",
      "\n",
      "############################## 2 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5ddf344a804144803ce8e834a0812c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 30.928764760494232 Accuracy: 0.89640625\n",
      "Batch Loss: 60.75272077322006 Accuracy: 0.8971875\n",
      "Batch Loss: 90.18083705753088 Accuracy: 0.8990625\n",
      "Batch Loss: 122.78986214101315 Accuracy: 0.8965234375\n",
      "Batch Loss: 154.28778317570686 Accuracy: 0.89596875\n",
      "Train Loss: 218.07364953681827 Train Accuracy: 0.8722996467979082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b71d92680c43b6a6f006410b7449c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.878874164932647\n",
      "\n",
      "\n",
      "############################## 3 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555929c62fd148409d6264992a0c649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 22.464916042983532 Accuracy: 0.92640625\n",
      "Batch Loss: 45.52330815792084 Accuracy: 0.92453125\n",
      "Batch Loss: 70.19599305838346 Accuracy: 0.92171875\n",
      "Batch Loss: 94.41127549856901 Accuracy: 0.920546875\n",
      "Batch Loss: 120.42761645466089 Accuracy: 0.9185625\n",
      "Train Loss: 192.87014197309813 Train Accuracy: 0.8869753306135859\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bffbf4a5c2429aaab00b3019fed348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8746030007666192\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "model.train()\n",
    "for i in range(3):\n",
    "    print('#'*30,i+1,'epoch start','#'*30)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.cuda()\n",
    "        y_pred = model(input_ids_batch.cuda(), attention_mask=attention_masks_batch.cuda())[0]\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        batches += 1\n",
    "        if batches % 100 == 0:\n",
    "            print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.cpu().numpy() / total)\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.cpu().numpy() / total)\n",
    "    print(\"Train Loss:\", sum(losses) / len(losses), \"Train Accuracy:\", sum(accuracies) / len(accuracies))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "        y_batch = y_batch.cuda()\n",
    "        y_pred = model(input_ids_batch.cuda(), attention_mask=attention_masks_batch.cuda())[0]\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        test_correct += (predicted == y_batch).sum()\n",
    "        test_total += len(y_batch)\n",
    "\n",
    "    print(\"valid  Accuracy:\", test_correct.cpu().numpy() / test_total)\n",
    "    print()\n",
    "    print()\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'] = 0\n",
    "test_dataset = koelectradataset(test,max_len,tok)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>유튜브 내달 2일 까지 크리에이터 지원 공간 운영</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어버이날 맑다 흐려지다 남부 지방 옅다 황사</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>내년 국가 RD 평가 때 논문 건수 반영 않다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>김명자 신임 과총 회장 원로 젊다 과학자 지혜 모으다 것</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>회색 인간 작가 김 동식 양심 고백 등 새 소설 집 2 권 추다 간</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>인천 오후 3시 35분 대설주의보 눈 3.1 cm 쌓이다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>노래방 지인 성추행 외교부 사무관 불구속 입건 종합</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>40년 전 부마항쟁 부산 시위 사진 2 점 최초 공개</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>게시판 아리랑 TV 아프리카 개발 은행 총회 개회 식 생중계</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9130</th>\n",
       "      <td>유영민 과 기 장관 강소특구 지역 혁신 중심 지원 책 강구</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  pred\n",
       "0               유튜브 내달 2일 까지 크리에이터 지원 공간 운영     0\n",
       "1                  어버이날 맑다 흐려지다 남부 지방 옅다 황사     0\n",
       "2                 내년 국가 RD 평가 때 논문 건수 반영 않다     0\n",
       "3           김명자 신임 과총 회장 원로 젊다 과학자 지혜 모으다 것     0\n",
       "4     회색 인간 작가 김 동식 양심 고백 등 새 소설 집 2 권 추다 간     0\n",
       "...                                     ...   ...\n",
       "9126        인천 오후 3시 35분 대설주의보 눈 3.1 cm 쌓이다     0\n",
       "9127           노래방 지인 성추행 외교부 사무관 불구속 입건 종합     0\n",
       "9128          40년 전 부마항쟁 부산 시위 사진 2 점 최초 공개     0\n",
       "9129      게시판 아리랑 TV 아프리카 개발 은행 총회 개회 식 생중계     0\n",
       "9130       유영민 과 기 장관 강소특구 지역 혁신 중심 지원 책 강구     0\n",
       "\n",
       "[9131 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065f6fcf842547acbe5e59c10f25ad62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "    y_batch = y_batch.cuda()\n",
    "    y_pred = model(input_ids_batch.cuda(), attention_mask=attention_masks_batch.cuda())[0]\n",
    "    predicted = torch.argmax(y_pred, 1)\n",
    "    #test_correct += (predicted == y_batch).sum()\n",
    "    #test_total += len(y_batch)\n",
    "    pred.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9131"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['topic_idx'] = df\n",
    "sub['topic_idx'] = sub['topic_idx'].apply(lambda x : int(x))\n",
    "#sub.to_csv('hyup_전처리 x drop_out:0.7.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
