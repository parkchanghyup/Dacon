{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fq0iuH16I3Ea"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LWkJEC74I86h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from adamp import AdamP\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import random\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = True  \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeOpHdoQI_7q"
   },
   "source": [
    "\n",
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wcjn-PdhI83N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45654, 3)\n",
      "(9131, 2)\n"
     ]
    }
   ],
   "source": [
    "# 학습용 데이터셋 불러오기\n",
    "import pandas as pd\n",
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Geq97KhbJFzp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천→핀란드 항공기 결항…휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                             title  topic_idx\n",
       "0      0          인천→핀란드 항공기 결항…휴가철 여행객 분통          4\n",
       "1      1    실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화          4\n",
       "2      2    이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3      3  NYT 클린턴 측근韓기업 특수관계 조명…공과 사 맞물려종합          4\n",
       "4      4         시진핑 트럼프에 중미 무역협상 조속 타결 희망          4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eelb1JYGI80K"
   },
   "outputs": [],
   "source": [
    "# index 컬럼 제거\n",
    "train = train.iloc[:,1:]\n",
    "test = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvxTO2o4JQe9"
   },
   "source": [
    "\n",
    "## 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Isu28t2PI8tm"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ppTusUVII8nt"
   },
   "outputs": [],
   "source": [
    "# 형태소 분석기(Okt) 불러오기 \n",
    "okt=Okt() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_YXBVMFWI8kb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 조사, 어미, 구두점 제거\\ndef func(text):\\n    clean = []\\n    for word in okt.pos(text, stem=True): #어간 추출\\n        if word[1] not in [\\'Josa\\', \\'Eomi\\', \\'Punctuation\\']: #조사, 어미, 구두점 제외 \\n            clean.append(word[0])\\n    \\n    \\n    return \" \".join(clean) \\n\\ntrain[\\'title\\'] = train[\\'title\\'].apply(lambda x : func(x))\\ntest[\\'title\\'] = test[\\'title\\'].apply(lambda x : func(x))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# 조사, 어미, 구두점 제거\n",
    "def func(text):\n",
    "    clean = []\n",
    "    for word in okt.pos(text, stem=True): #어간 추출\n",
    "        if word[1] not in ['Josa', 'Eomi', 'Punctuation']: #조사, 어미, 구두점 제외 \n",
    "            clean.append(word[0])\n",
    "    \n",
    "    \n",
    "    return \" \".join(clean) \n",
    "\n",
    "train['title'] = train['title'].apply(lambda x : func(x))\n",
    "test['title'] = test['title'].apply(lambda x : func(x))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1K_EIHOlJWvz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: 36523\n",
      "valid shape is: 9131\n"
     ]
    }
   ],
   "source": [
    "# Train / Test set 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(\"train shape is:\", len(train))\n",
    "print(\"valid shape is:\", len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FErPSLykJZwC"
   },
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "drBV1__uJZBz"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n",
    "\n",
    "#         self.sentences = [transform([i[sent_idx]]) for i in dataset.to_numpy()]\n",
    "        self.token_ids = []\n",
    "        self.valid_length =[]\n",
    "        self.segment_ids = []\n",
    "        for i in dataset.to_numpy():\n",
    "            out = transform([i[sent_idx]])\n",
    "            self.token_ids.append(out[0])\n",
    "            self.valid_length.append(out[1])\n",
    "            self.segment_ids.append(out[2])\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset.to_numpy()]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        token_ids = self.token_ids[i]\n",
    "        valid_length = self.valid_length[i]\n",
    "        segment_ids = self.segment_ids[i]\n",
    "        return {\n",
    "            'token_ids' : torch.tensor(token_ids, dtype=torch.long),\n",
    "            'valid_length' : torch.tensor(valid_length, dtype=torch.long),\n",
    "            'segment_ids' : torch.tensor(segment_ids, dtype=torch.long),\n",
    "            'label' : torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYmaF7NiJofy"
   },
   "source": [
    "\n",
    "## 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "h2zsmQ_bJqDM"
   },
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 48 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 128\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXT55uXIJtKK"
   },
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BwIw4mxfI8g7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# 기본 Bert tokenizer 사용\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0uE3uUT0I8dg"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 7, # softmax 사용 <- binary일 경우는 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        bert_output = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        pooler = bert_output['pooler_output']\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "      \n",
    "model = BERTClassifier(bertmodel, dr_rate=0.7).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tIqXzwLuJ3ri"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(valid, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "# pytorch용 DataLoader 사용\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxP11pmWJ39H"
   },
   "source": [
    "\n",
    "## 학습에 필요한 파라미터 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "66WYDUN9I8Yl"
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 옵티마이저 선언\n",
    "optimizer = AdamP(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "\n",
    "\n",
    "#scheduler\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4J18wLEpI8SA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZt7sFwHKneK"
   },
   "source": [
    "\n",
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OiWaV-NuI8KD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724fdae4cd8f443f817431cb14d9ea49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.0181121826171875 train acc 0.1484375\n",
      "epoch 1 batch id 201 loss 0.29367995262145996 train acc 0.636738184079602\n",
      "epoch 1 train acc 0.705152768742885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6aa7484e374eb6b667397038c66469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.8803244105297158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad00812bda64e3baeb0e091fd5ee7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.3361748456954956 train acc 0.875\n",
      "epoch 2 batch id 201 loss 0.19312375783920288 train acc 0.8927238805970149\n",
      "epoch 2 train acc 0.8986579372662222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfedb075699d47a0961b4e3bb862422c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.8904105095284238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0e5b87f64e477c81e9fe235ec78fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.1944151520729065 train acc 0.9375\n",
      "epoch 3 batch id 201 loss 0.13997071981430054 train acc 0.9338075248756219\n",
      "epoch 3 train acc 0.9373646883639617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213a9287a8c04cf49d93cca87f21c485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.8891084261950906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aab35fe87d44ec6b6b1dffeb8772ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.09061294794082642 train acc 0.9765625\n",
      "epoch 4 batch id 201 loss 0.06744234263896942 train acc 0.9632695895522388\n",
      "epoch 4 train acc 0.9662642045454546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73386ab0fda9448eb5f9679a79ac55c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.8884599079457365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3445bb6fa50b4ebfaad7d1b7fdce843a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.046438053250312805 train acc 0.9921875\n",
      "epoch 5 batch id 201 loss 0.03527696430683136 train acc 0.9808379975124378\n",
      "epoch 5 train acc 0.9818072552447552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c05e19b630845c6aa5f8aad95728a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.8880258801679587\n"
     ]
    }
   ],
   "source": [
    "valid_acc_max = 0\n",
    "# 모델 학습 시작\n",
    "model.train()\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    epoch_num+=1\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    for batch_id, items in tqdm_dataset:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        token_ids = items['token_ids'].cuda()\n",
    "        segment_ids = items['segment_ids'].cuda()\n",
    "        valid_length= items['valid_length']\n",
    "        label = items['label'].cuda()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            loss = loss_fn(out, label)\n",
    "        #loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        #optimizer.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "\n",
    "\n",
    "    model.eval() # 평가 모드로 변경\n",
    "    tqdm_dataset = tqdm(enumerate(test_dataloader))\n",
    "    for batch_id, items in tqdm_dataset:\n",
    "        token_ids = items['token_ids'].cuda()\n",
    "        segment_ids = items['segment_ids'].cuda()\n",
    "        valid_length= items['valid_length']\n",
    "        label = items['label'].cuda()\n",
    "        with torch.no_grad():\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxx60X3cKrUj"
   },
   "source": [
    "\n",
    "## test 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9I4MHIMII8EX"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "s3XdB1ZRI73T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   500  of  9,131.    Elapsed: 0:00:05.\n",
      "  Batch 1,000  of  9,131.    Elapsed: 0:00:11.\n",
      "  Batch 1,500  of  9,131.    Elapsed: 0:00:16.\n",
      "  Batch 2,000  of  9,131.    Elapsed: 0:00:22.\n",
      "  Batch 2,500  of  9,131.    Elapsed: 0:00:27.\n",
      "  Batch 3,000  of  9,131.    Elapsed: 0:00:33.\n",
      "  Batch 3,500  of  9,131.    Elapsed: 0:00:38.\n",
      "  Batch 4,000  of  9,131.    Elapsed: 0:00:44.\n",
      "  Batch 4,500  of  9,131.    Elapsed: 0:00:49.\n",
      "  Batch 5,000  of  9,131.    Elapsed: 0:00:54.\n",
      "  Batch 5,500  of  9,131.    Elapsed: 0:01:00.\n",
      "  Batch 6,000  of  9,131.    Elapsed: 0:01:05.\n",
      "  Batch 6,500  of  9,131.    Elapsed: 0:01:11.\n",
      "  Batch 7,000  of  9,131.    Elapsed: 0:01:16.\n",
      "  Batch 7,500  of  9,131.    Elapsed: 0:01:22.\n",
      "  Batch 8,000  of  9,131.    Elapsed: 0:01:27.\n",
      "  Batch 8,500  of  9,131.    Elapsed: 0:01:32.\n",
      "  Batch 9,000  of  9,131.    Elapsed: 0:01:38.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "# 테스트 데이터 예측\n",
    "\n",
    "#시작 시간 설정\n",
    "t0 = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "for step in range(len(test)):\n",
    "    if step % 500 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step,len(test), elapsed))\n",
    "    test_sentence = test.title[step]\n",
    "    test_label = 0\n",
    "\n",
    "\n",
    "    unseen_test = pd.DataFrame([[test_sentence, test_label]], columns = [['title', 'topic_idx']])\n",
    "    #unseen_values = unseen_test.values\n",
    "    test_set = BERTDataset(unseen_test, 0, 1, tok, max_len, True, False)\n",
    "    test_input = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    for batch_id, items in enumerate(test_input):\n",
    "        token_ids = items['token_ids'].cuda()\n",
    "        segment_ids = items['segment_ids'].cuda()\n",
    "        valid_length= items['valid_length']\n",
    "        #label = items['label'].to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        pred.append(int(torch.argmax(out).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkmHpnBDLB8M"
   },
   "source": [
    "\n",
    "## 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LTJa8dbcLC6N"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['topic_idx'] = pred\n",
    "sub.to_csv('kobert_epcoh5.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45654</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45656</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  topic_idx\n",
       "0  45654          3\n",
       "1  45655          3\n",
       "2  45656          2\n",
       "3  45657          2\n",
       "4  45658          3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8552026286966046\n",
      "5 번째 완료\n"
     ]
    }
   ],
   "source": [
    "sol =  pd.read_csv('solution_sample.csv')\n",
    "#sol2 = pd.read_csv('solution.csv')\n",
    "\n",
    "sub2 = pred[0:len(sol)]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(sol.topic_idx, sub2))\n",
    "print(epoch_num,\"번째 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "kobert",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
