{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TjWpsF0ETFQH"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OySQthgzTF_N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from adamp import AdamP\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eH7vLX0YTGuW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = True  \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6NzVrOU7THrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45654, 3)\n",
      "(9131, 2)\n"
     ]
    }
   ],
   "source": [
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IIgvX5xRTKZB"
   },
   "outputs": [],
   "source": [
    "train = train.iloc[:,1:]\n",
    "test = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WZQruEoETLFN"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "# 형태소 분석기(Okt) 불러오기 \n",
    "okt=Okt() \n",
    "\n",
    "# 조사, 어미, 구두점 제거\n",
    "def func(text):\n",
    "    clean = []\n",
    "    for word in okt.pos(text, stem=True): #어간 추출\n",
    "        if word[1] not in ['Josa', 'Eomi', 'Punctuation']: #조사, 어미, 구두점 제외 \n",
    "            clean.append(word[0])\n",
    "    \n",
    "    \n",
    "    return \" \".join(clean) \n",
    "\n",
    "train['title'] = train['title'].apply(lambda x : func(x))\n",
    "test['title'] = test['title'].apply(lambda x : func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDpfzsuhTOc-"
   },
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mS98ToF6TN_R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer ,AutoModelForSequenceClassification\n",
    "\n",
    "#model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tok = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-large',num_labels=7).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LC3TT2j5TRER"
   },
   "outputs": [],
   "source": [
    "class koelectradataset(Dataset):  \n",
    "    def __init__(self, dataset,max_len,bert_tokenizer):\n",
    "        \n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.dataset = dataset\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, :2].values\n",
    "        text = row[0]\n",
    "        y = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "    \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt3x4SeMTSMO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## 1 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebdb283dac84c77955b29a9f2ff34e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 64.02468879520893 Accuracy: 0.7959375\n",
      "Batch Loss: 108.77728089690208 Accuracy: 0.828828125\n",
      "Batch Loss: 149.7535834312439 Accuracy: 0.8419270833333333\n",
      "Batch Loss: 189.14376704394817 Accuracy: 0.848515625\n",
      "Train Loss: 219.60538905858994 Train Accuracy: 0.8508673938756736\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a8ea57b5304345a115de644beaf6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8655539492706006\n",
      "\n",
      "\n",
      "############################## 2 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2017a21e5843f1988a94b5e906cedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 30.866082824766636 Accuracy: 0.89953125\n",
      "Batch Loss: 61.843560107052326 Accuracy: 0.899140625\n",
      "Batch Loss: 92.41992459446192 Accuracy: 0.8990104166666667\n",
      "Batch Loss: 123.21880756318569 Accuracy: 0.8984765625\n",
      "Train Loss: 183.7776500955224 Train Accuracy: 0.8740636088842161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3df8581b884f4f8568b6445caeab28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8740964647128401\n",
      "\n",
      "\n",
      "############################## 3 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0e272b54254e7ab584e1f59a76fadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 19.56812298670411 Accuracy: 0.93671875\n",
      "Batch Loss: 42.760465770959854 Accuracy: 0.9303125\n",
      "Batch Loss: 68.19061571359634 Accuracy: 0.9236979166666667\n",
      "Batch Loss: 90.77722203731537 Accuracy: 0.923359375\n",
      "Train Loss: 159.0399935543537 Train Accuracy: 0.8899220221667324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0eb03b36734a1f99d3e2f655eb5704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8685766855040085\n",
      "\n",
      "\n",
      "############################## 4 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742e00d59a874996b52c49f80c5cfdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 20.84789026528597 Accuracy: 0.93375\n",
      "Batch Loss: 46.123411886394024 Accuracy: 0.927890625\n",
      "Batch Loss: 70.66186838597059 Accuracy: 0.924375\n",
      "Batch Loss: 95.43429301679134 Accuracy: 0.9237109375\n",
      "Train Loss: 147.71613958850503 Train Accuracy: 0.8983851360231305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6301549e0a14ae684be248b1b0a90c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8636483112104087\n",
      "\n",
      "\n",
      "############################## 5 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb0369c15844862a32406f670595d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 16.849375542253256 Accuracy: 0.9459375\n",
      "Batch Loss: 34.13037687726319 Accuracy: 0.94640625\n",
      "Batch Loss: 52.39595231972635 Accuracy: 0.9459895833333334\n",
      "Batch Loss: 68.91681771166623 Accuracy: 0.9462890625\n",
      "Train Loss: 137.95186520554125 Train Accuracy: 0.9059534761466684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1222a042bcd6471ca1e896f73c276c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8372979366539625\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## 1 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc03c9bd2f44ad888ac0027b7de8fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 196.15365481376648 Accuracy: 0.1603125\n",
      "Batch Loss: 391.5454224348068 Accuracy: 0.161953125\n",
      "Batch Loss: 586.2408883571625 Accuracy: 0.16302083333333334\n",
      "Batch Loss: 781.2093926668167 Accuracy: 0.162265625\n",
      "Train Loss: 929.4725106954575 Train Accuracy: 0.15912077802602181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d0baddcd9341deb088403400f7cc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.1612564068865817\n",
      "\n",
      "\n",
      "############################## 2 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56821ae96f5b47f1b3659b273cf26ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 194.75771057605743 Accuracy: 0.1653125\n",
      "Batch Loss: 389.025763630867 Accuracy: 0.166875\n",
      "Batch Loss: 583.8905665874481 Accuracy: 0.16270833333333334\n",
      "Batch Loss: 778.3510016202927 Accuracy: 0.1609375\n",
      "Train Loss: 927.7241632938385 Train Accuracy: 0.16002431331318176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a06026e35a4c579fac657d9c27a352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.1671047443816533\n",
      "\n",
      "\n",
      "############################## 3 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4d6c08f6294a1d86f8194794aa5b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 194.25445699691772 Accuracy: 0.17015625\n",
      "Batch Loss: 388.54089307785034 Accuracy: 0.16078125\n",
      "Batch Loss: 582.9396605491638 Accuracy: 0.15786458333333334\n",
      "Batch Loss: 777.335245013237 Accuracy: 0.1600390625\n",
      "Train Loss: 926.8247445027033 Train Accuracy: 0.16000788539886976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3b94ed44f440c190759dcaf12ca3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.1671047443816533\n",
      "\n",
      "\n",
      "############################## 4 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a426ac110b704d46ba16bfb951b7d474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 194.39227545261383 Accuracy: 0.158125\n",
      "Batch Loss: 389.0204463005066 Accuracy: 0.155\n",
      "Batch Loss: 583.2901132106781 Accuracy: 0.1553125\n",
      "Batch Loss: 777.4728757143021 Accuracy: 0.158046875\n",
      "Train Loss: 926.3190554082394 Train Accuracy: 0.16000788539886976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5f204bba4242588b933f71431d579f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.1671047443816533\n",
      "\n",
      "\n",
      "############################## 5 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0653f425c190423191528f44b9841195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 194.18914270401 Accuracy: 0.160625\n",
      "Batch Loss: 388.53126060962677 Accuracy: 0.158359375\n",
      "Batch Loss: 582.598895072937 Accuracy: 0.16010416666666666\n",
      "Batch Loss: 776.790965795517 Accuracy: 0.162265625\n",
      "Train Loss: 925.9321939945221 Train Accuracy: 0.16001445656459456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c71c3079b094da2a439cbfcee68f90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.15185963990011828\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## 1 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa1daeeb4724218aa68e49d261a809c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 61.675462767481804 Accuracy: 0.803125\n",
      "Batch Loss: 106.65061494708061 Accuracy: 0.830625\n",
      "Batch Loss: 147.9585588723421 Accuracy: 0.8406770833333334\n",
      "Batch Loss: 188.59805412590504 Accuracy: 0.8475390625\n",
      "Train Loss: 217.57528260350227 Train Accuracy: 0.8517545012485215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fb45e5b9934bce8b1569767838496d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.8664739124720725\n",
      "\n",
      "\n",
      "############################## 2 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3230a981754408be992f166e1ee792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 30.314347498118877 Accuracy: 0.9021875\n",
      "Batch Loss: 60.11511919647455 Accuracy: 0.900546875\n",
      "Batch Loss: 225.60364639014006 Accuracy: 0.7050520833333334\n",
      "Batch Loss: 420.5855315849185 Accuracy: 0.5671875\n",
      "Train Loss: 393.5549966804683 Train Accuracy: 0.6754501248521487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0152140e1fc04752849a5de20ed4ec73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid  Accuracy: 0.15185963990011828\n",
      "\n",
      "\n",
      "############################## 3 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974edb85a6aa4338af14762a8c8651ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# dirty_mnist_answer에서 train_idx와 val_idx를 생성\n",
    "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
    "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(train['title'],train['topic_idx']),1):\n",
    "    \n",
    "    # cuda cache 초기화\n",
    "    torch.cuda.empty_cache()\n",
    "    train_data = train.loc[trn_idx]\n",
    "    valid_data = train.loc[val_idx]\n",
    "    \n",
    "    max_len = 32\n",
    "    batch_size = 64 \n",
    "    num_epochs = 10\n",
    "    warmup_ratio = 0.1\n",
    "    num_epochs = 5\n",
    "    max_grad_norm = 1\n",
    "    log_interval = 200\n",
    "    learning_rate = 5e-5\n",
    "    \n",
    "    train_dataset = koelectradataset(train_data,max_len,tok)\n",
    "    test_dataset = koelectradataset(valid_data,max_len,tok)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-large',num_labels=7).cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    # 옵티마이저 선언\n",
    "    optimizer = AdamP(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    model.train()\n",
    "    valid_acc_max = 0\n",
    "    for i in range(num_epochs):\n",
    "        print('#'*30,i+1,'epoch start','#'*30)\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batches = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_batch = y_batch.cuda()\n",
    "            y_pred = model(input_ids_batch.cuda(), attention_mask=attention_masks_batch.cuda())[0]\n",
    "            loss = F.cross_entropy(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted = torch.argmax(y_pred, 1)\n",
    "            correct += (predicted == y_batch).sum()\n",
    "            total += len(y_batch)\n",
    "\n",
    "            batches += 1\n",
    "            if batches % 100 == 0:\n",
    "                print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.cpu().numpy() / total)\n",
    "\n",
    "        losses.append(total_loss)\n",
    "        accuracies.append(correct.cpu().numpy() / total)\n",
    "        print(\"Train Loss:\", sum(losses) / len(losses), \"Train Accuracy:\", sum(accuracies) / len(accuracies))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "            y_batch = y_batch.cuda()\n",
    "            y_pred = model(input_ids_batch.cuda(), attention_mask=attention_masks_batch.cuda())[0]\n",
    "            predicted = torch.argmax(y_pred, 1)\n",
    "            test_correct += (predicted == y_batch).sum()\n",
    "            test_total += len(y_batch)\n",
    "        test_acc = test_correct.cpu().numpy() / test_total\n",
    "        print(\"valid  Accuracy:\",test_acc )\n",
    "        print()\n",
    "        print()\n",
    "                # 모델 저장\n",
    "        if valid_acc_max < test_acc:\n",
    "            valid_acc_max = test_acc\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "    # 폴드별로 가장 좋은 모델 저장\n",
    "    best_models.append(best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,model in enumerate(best_models):\n",
    "    torch.save(model.state_dict(), 'model/koelectra_'+str(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYMquav63cuM"
   },
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForSequenceClassification, ElectraTokenizer\n",
    "\n",
    "#model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tok = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\",num_labels = 7).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = []\n",
    "model = model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\",num_labels = 7).cuda()\n",
    "for i in range(3):\n",
    "    model.load_state_dict(torch.load('model/koelectra_'+str(i)))\n",
    "    best_model.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ACju8-g53Z53"
   },
   "outputs": [],
   "source": [
    "max_len = 32\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5\n",
    "\n",
    "test['pred'] = 0\n",
    "test_dataset = koelectradataset(test,max_len,tok)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 1 : 번째 모델 예측 진행 ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff05d36162624208b1168a69ee8528fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 2 : 번째 모델 예측 진행 ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911309a8c8fc40a3963c5a385c5c92a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 3 : 번째 모델 예측 진행 ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0eaff1e6e0b48c1a907552711ecccbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 4 : 번째 모델 예측 진행 ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b84109c8b44bd5aa38b6d3694990bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## 5 : 번째 모델 예측 진행 ##########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019721e6168445d4b834a5e25b90507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 평가모드로 변경\n",
    "preds = []\n",
    "for idx,Best_Model in enumerate(best_models):\n",
    "    print('#'*10,idx+1,\": 번째 모델 예측 진행\",'#'*10)\n",
    "    model = Best_Model\n",
    "    model.eval()\n",
    "    \n",
    "    pred = []\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "        y_batch = y_batch.cuda()\n",
    "        y_pred = model(input_ids_batch.cuda(), attention_mask=attention_masks_batch.cuda())[0]\n",
    "        #predicted = torch.argmax(y_pred, 1)\n",
    "        #test_correct += (predicted == y_batch).sum()\n",
    "        #test_total += len(y_batch)\n",
    "        pred.extend(y_pred.cpu().detach().numpy().tolist())\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9131"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = []\n",
    "for i in range(len(preds[0])):\n",
    "    a = np.array(preds[0][i])\n",
    "    b = np.array(preds[1][i])\n",
    "    c = np.array(preds[2][i])\n",
    "    d = np.array(preds[3][i])\n",
    "    e = np.array(preds[4][i])\n",
    "\n",
    "    \n",
    "    new_pred.append(a+b+c+d+e)\n",
    "    \n",
    "len(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(new_pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub.topic_idx = pred"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "koelectra",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
