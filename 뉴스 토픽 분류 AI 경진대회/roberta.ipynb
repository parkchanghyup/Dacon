{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from adamp import AdamP\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu111'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45654, 3)\n",
      "(9131, 2)\n"
     ]
    }
   ],
   "source": [
    "# 학습용 데이터셋 불러오기\n",
    "import pandas as pd\n",
    "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:,1:]\n",
    "test = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기(Okt) 불러오기 \n",
    "okt=Okt() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조사, 어미, 구두점 제거\n",
    "def func(text):\n",
    "    clean = []\n",
    "    for word in okt.pos(text, stem=True): #어간 추출\n",
    "        if word[1] not in ['Josa', 'Eomi', 'Punctuation']: #조사, 어미, 구두점 제외 \n",
    "            clean.append(word[0])\n",
    "    \n",
    "    \n",
    "    return \" \".join(clean) \n",
    "\n",
    "train['title'] = train['title'].apply(lambda x : func(x))\n",
    "test['title'] = test['title'].apply(lambda x : func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: 36523\n",
      "valid shape is: 9131\n"
     ]
    }
   ],
   "source": [
    "# Train / Test set 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(\"train shape is:\", len(train))\n",
    "print(\"valid shape is:\", len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "#model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tok = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class koelectradataset(Dataset):  \n",
    "    def __init__(self, dataset,max_len,bert_tokenizer):\n",
    "        \n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.dataset = dataset\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, :2].values\n",
    "        text = row[0]\n",
    "        y = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "    \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "train_dataset = koelectradataset(train,max_len,tok)\n",
    "test_dataset = koelectradataset(valid,max_len,tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-large',num_labels=7).to(device)\n",
    "#model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# 옵티마이저 선언\n",
    "optimizer = AdamP(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## 1 epoch start ##############################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa026b1d9e6246e3b3ebd994cc165fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacon/anaconda3/envs/pch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2126: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 129.9117430150509 Accuracy: 0.51890625\n",
      "Batch Loss: 178.63578420877457 Accuracy: 0.679296875\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    print('#'*30,i+1,'epoch start','#'*30)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        batches += 1\n",
    "        if batches % 100 == 0:\n",
    "            print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.cpu().numpy() / total)\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.cpu().numpy() / total)\n",
    "    print(\"Train Loss:\", sum(losses) / len(losses), \"Train Accuracy:\", sum(accuracies) / len(accuracies))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        predicted = torch.argmax(y_pred, 1)\n",
    "        test_correct += (predicted == y_batch).sum()\n",
    "        test_total += len(y_batch)\n",
    "\n",
    "    print(\"valid  Accuracy:\", test_correct.cpu().numpy() / test_total)\n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred'] = 0\n",
    "test_dataset = koelectradataset(test,max_len,tok)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>유튜브 내달 2일까지 크리에이터 지원 공간 운영</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어버이날 맑다가 흐려져…남부지방 옅은 황사</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>내년부터 국가RD 평가 때 논문건수는 반영 않는다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>인천 오후 3시35분 대설주의보…눈 3.1cm 쌓여</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>노래방에서 지인 성추행 외교부 사무관 불구속 입건종합</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>40년 전 부마항쟁 부산 시위 사진 2점 최초 공개</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>게시판 아리랑TV 아프리카개발은행 총회 개회식 생중계</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9130</th>\n",
       "      <td>유영민 과기장관 강소특구는 지역 혁신의 중심…지원책 강구</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  pred\n",
       "0          유튜브 내달 2일까지 크리에이터 지원 공간 운영     0\n",
       "1             어버이날 맑다가 흐려져…남부지방 옅은 황사     0\n",
       "2         내년부터 국가RD 평가 때 논문건수는 반영 않는다     0\n",
       "3     김명자 신임 과총 회장 원로와 젊은 과학자 지혜 모을 것     0\n",
       "4      회색인간 작가 김동식 양심고백 등 새 소설집 2권 출간     0\n",
       "...                               ...   ...\n",
       "9126     인천 오후 3시35분 대설주의보…눈 3.1cm 쌓여     0\n",
       "9127    노래방에서 지인 성추행 외교부 사무관 불구속 입건종합     0\n",
       "9128     40년 전 부마항쟁 부산 시위 사진 2점 최초 공개     0\n",
       "9129    게시판 아리랑TV 아프리카개발은행 총회 개회식 생중계     0\n",
       "9130  유영민 과기장관 강소특구는 지역 혁신의 중심…지원책 강구     0\n",
       "\n",
       "[9131 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba2f97664754e9fb95db7a40663bb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacon/anaconda3/envs/pch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2126: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    predicted = torch.argmax(y_pred, 1)\n",
    "    #test_correct += (predicted == y_batch).sum()\n",
    "    #test_total += len(y_batch)\n",
    "    pred.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9131"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['topic_idx'] = df\n",
    "sub['topic_idx'] = sub['topic_idx'].apply(lambda x : int(x))\n",
    "sub.to_csv('hyup_전처리 x drop_out:0.7.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
