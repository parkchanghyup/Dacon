{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "dongseo_pytorch_(1) (1).ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unnLMSOz6xyl"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSQnkpgo6xyn"
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import lightgbm as lgb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJXUkvBf6xyo"
      },
      "source": [
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGhU8h496xyo"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_is-ila56xyo",
        "outputId": "8cf890e7-8c70-48e9-bd07-7d931073c86f"
      },
      "source": [
        "!wget 'https://bit.ly/3dD5MU9'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('3dD5MU9', 'r') as existing_zip:\n",
        "    existing_zip.extractall('data')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-03 14:44:31--  https://bit.ly/3dD5MU9\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://drive.google.com/uc?export=download&id=1kkF00wW8v0npJ8S2nA7--eMTH3gOL03z [following]\n",
            "--2021-06-03 14:44:31--  https://drive.google.com/uc?export=download&id=1kkF00wW8v0npJ8S2nA7--eMTH3gOL03z\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.132.113, 74.125.132.101, 74.125.132.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.132.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qu68qqt0usqesc5oora9n23fotj6ttui/1622731425000/00192245294648390361/*/1kkF00wW8v0npJ8S2nA7--eMTH3gOL03z?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-06-03 14:44:36--  https://doc-14-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qu68qqt0usqesc5oora9n23fotj6ttui/1622731425000/00192245294648390361/*/1kkF00wW8v0npJ8S2nA7--eMTH3gOL03z?e=download\n",
            "Resolving doc-14-94-docs.googleusercontent.com (doc-14-94-docs.googleusercontent.com)... 142.251.6.132, 2607:f8b0:4001:c5a::84\n",
            "Connecting to doc-14-94-docs.googleusercontent.com (doc-14-94-docs.googleusercontent.com)|142.251.6.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-zip-compressed]\n",
            "Saving to: ‘3dD5MU9’\n",
            "\n",
            "3dD5MU9                 [ <=>                ]   2.07M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-06-03 14:44:37 (146 MB/s) - ‘3dD5MU9’ saved [2172031]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gKkokbGN6xyp",
        "outputId": "4766fa33-1ff2-4dd1-b7cc-34fe90b0ad34"
      },
      "source": [
        "import pandas as pd\n",
        "energy = pd.read_csv('data/energy.csv')\n",
        "dangjin_fcst = pd.read_csv('./data/dangjin_fcst_data.csv')\n",
        "ulsan_fcst = pd.read_csv('./data/ulsan_fcst_data.csv')\n",
        "pd.read_csv('data/dangjin_fcst_data.csv')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Forecast time</th>\n",
              "      <th>forecast</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Humidity</th>\n",
              "      <th>WindSpeed</th>\n",
              "      <th>WindDirection</th>\n",
              "      <th>Cloud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-03-01 11:00:00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>7.3</td>\n",
              "      <td>309.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-03-01 11:00:00</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>7.1</td>\n",
              "      <td>314.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-03-01 11:00:00</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>323.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-03-01 11:00:00</td>\n",
              "      <td>13.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>336.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-03-01 11:00:00</td>\n",
              "      <td>16.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>5.5</td>\n",
              "      <td>339.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162203</th>\n",
              "      <td>2021-03-01 08:00:00</td>\n",
              "      <td>52.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>187.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162204</th>\n",
              "      <td>2021-03-01 08:00:00</td>\n",
              "      <td>55.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>217.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162205</th>\n",
              "      <td>2021-03-01 08:00:00</td>\n",
              "      <td>58.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>210.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162206</th>\n",
              "      <td>2021-03-01 08:00:00</td>\n",
              "      <td>61.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>164.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162207</th>\n",
              "      <td>2021-03-01 08:00:00</td>\n",
              "      <td>64.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>152.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>162208 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Forecast time  forecast  ...  WindDirection  Cloud\n",
              "0       2018-03-01 11:00:00       4.0  ...          309.0    2.0\n",
              "1       2018-03-01 11:00:00       7.0  ...          314.0    1.0\n",
              "2       2018-03-01 11:00:00      10.0  ...          323.0    1.0\n",
              "3       2018-03-01 11:00:00      13.0  ...          336.0    1.0\n",
              "4       2018-03-01 11:00:00      16.0  ...          339.0    1.0\n",
              "...                     ...       ...  ...            ...    ...\n",
              "162203  2021-03-01 08:00:00      52.0  ...          187.0    1.0\n",
              "162204  2021-03-01 08:00:00      55.0  ...          217.0    1.0\n",
              "162205  2021-03-01 08:00:00      58.0  ...          210.0    1.0\n",
              "162206  2021-03-01 08:00:00      61.0  ...          164.0    1.0\n",
              "162207  2021-03-01 08:00:00      64.0  ...          152.0    1.0\n",
              "\n",
              "[162208 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-fDH06m6xyp"
      },
      "source": [
        "## 데이터 병합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecWQGRS36xyq"
      },
      "source": [
        "def to_date(x):\n",
        "    return timedelta(hours=x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMMXnDhS6xyq"
      },
      "source": [
        "def generate_df(df_):\n",
        "    df = df_.copy()\n",
        "\n",
        "    df['Forecast_time'] = pd.to_datetime(df['Forecast time'])\n",
        "\n",
        "    # 20 시\n",
        "    a = df[df[\"forecast\"] == 4.0]\n",
        "    a = a[a[\"Forecast time\"].apply(lambda x: \"20:00:00\" in x)]\n",
        "    a.loc[:, 'Forecast_time'] = a.loc[:, 'Forecast_time'] + a.loc[:, 'forecast'].map(to_date)\n",
        "\n",
        "    # 23 시\n",
        "    b = df[df[\"forecast\"] <= 22]\n",
        "    b = b[b[\"Forecast time\"].apply(lambda x: \"23:00:00\" in x)]\n",
        "    b.loc[:, 'Forecast_time'] = b.loc[:, 'Forecast_time'] + b.loc[:, 'forecast'].map(to_date)\n",
        "\n",
        "    # 병합\n",
        "    c = pd.concat([a, b])\n",
        "    print(f\"20시 사용 데이터 길이 : {len(a)}\")\n",
        "    print(f\"23시 사용 데이터 길이 : {len(b)}\")\n",
        "    print(f\"합친 데이터 길이 : {len(c)}\")\n",
        "    print()\n",
        "\n",
        "    # 정렬\n",
        "    c.sort_values(by=['Forecast_time'], inplace=True)\n",
        "    c = c[['Forecast_time', 'Temperature', 'Humidity', 'WindSpeed', 'WindDirection', 'Cloud']]\n",
        "\n",
        "    return c"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Ls4pPj6xyq",
        "outputId": "f6342976-a4b7-4239-85df-0b3c2b2ea474"
      },
      "source": [
        "dangjin_filled = generate_df(dangjin_fcst)\n",
        "ulsan_filled = generate_df(ulsan_fcst)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20시 사용 데이터 길이 : 1096\n",
            "23시 사용 데이터 길이 : 7672\n",
            "합친 데이터 길이 : 8768\n",
            "\n",
            "20시 사용 데이터 길이 : 1096\n",
            "23시 사용 데이터 길이 : 7672\n",
            "합친 데이터 길이 : 8768\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG2A2zc66xyr"
      },
      "source": [
        "## 보간"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmVJwyNA6xyr"
      },
      "source": [
        "def interpolate_df(df, method='linear'):\n",
        "    new_df = pd.DataFrame()\n",
        "    new_df['Forecast_time'] = pd.date_range(start=df['Forecast_time'].iloc[0], end=df['Forecast_time'].iloc[-1], freq='H')\n",
        "    new_df = pd.merge(new_df, df, on='Forecast_time', how='outer')\n",
        "    return new_df.interpolate(method=method)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlCdMWWK6xyr"
      },
      "source": [
        "dangjin_interpolated = interpolate_df(dangjin_filled, method='linear')\n",
        "ulsan_interpolated = interpolate_df(ulsan_filled, method='linear')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZEP4N0w6xys"
      },
      "source": [
        "def train_datast(energy_df, fcst_df, target):\n",
        "    # 일기 예보 있는 날짜만 선택\n",
        "    energy = energy_df.loc[24:]\n",
        "    energy.index = range(energy.shape[0])\n",
        "    \n",
        "    # 발전량 데이터가 있는 날짜만 선택\n",
        "    fcst = fcst_df\n",
        "    fcst.index = range(fcst.shape[0])\n",
        "    \n",
        "    # 발전량과 일기예보 연결\n",
        "    concat_df = pd.concat([energy, fcst], axis=1)\n",
        "    \n",
        "    \n",
        "    # 예보 시간, 날짜, 기상 예보 및 발전량 선택\n",
        "    feature_df = concat_df[['Temperature', 'Humidity', 'WindSpeed', 'WindDirection', 'Cloud', target]]\n",
        "    feature_df.fillna(0,inplace = True)\n",
        "\n",
        "    return np.array(feature_df[:-22])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQWCFcMA6xys"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdG6uo_66xys"
      },
      "source": [
        "dangjin_interpolated['Forecast_time'] = dangjin_interpolated['Forecast_time'].astype('str')\n",
        "ulsan_interpolated['Forecast_time'] = ulsan_interpolated['Forecast_time'].astype('str')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78c1TGvc6xys",
        "outputId": "0f1dda9f-764c-4ebb-e44d-b7a9006b6c64"
      },
      "source": [
        "df = train_datast(energy, dangjin_interpolated, target='dangjin_floating')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  downcast=downcast,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIWEyvc66xys"
      },
      "source": [
        "## 하이퍼 파라미터 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhmgE7cP6xyt"
      },
      "source": [
        "# hyper parameters\n",
        "seq_length = 24 # 입력 시퀀스 길이\n",
        "data_dim = 6 # 입력 데이터 차원 (변수 갯수)\n",
        "hidden_dim = 30 #(출력 데이터의 차원)\n",
        "output_dim = 1 #(최종 예측 데이터 차원)\n",
        "learning_rate = 0.01\n",
        "iterations = 3000"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyOTzv746xyt",
        "outputId": "a4014857-c758-467c-efa5-62572fff6715"
      },
      "source": [
        "train_size = int(len(df) * 1)\n",
        "train_set = df[:train_size]\n",
        "test_set = df[train_size-672:]\n",
        "train_set.shape, test_set.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26280, 6), (672, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaCViZ9HukrM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEnKAgcM6xyt"
      },
      "source": [
        "## MinMax 정규화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY8W9bGJ6xyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03904ace-d67d-422d-d6fd-f0bf0a19bf0f"
      },
      "source": [
        "'''scaler = MinMaxScaler()\n",
        "scaler.fit(train_set)\n",
        "train_set = scaler.transform(train_set)\n",
        "test_set = scaler.transform(test_set)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'scaler = MinMaxScaler()\\nscaler.fit(train_set)\\ntrain_set = scaler.transform(train_set)\\ntest_set = scaler.transform(test_set)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ83K20R6xyu"
      },
      "source": [
        "def make_sequence(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "\n",
        "    for i in range(0, len(time_series) - seq_length) :\n",
        "        _x = time_series[i:i + seq_length, : ] \n",
        "        _y = time_series[i+ seq_length, [-1]]\n",
        "        \n",
        "\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk7NCQEL6xyu"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True, bidirectional=True)\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim*2, 20),\n",
        "            torch.nn.Linear(20, 10),\n",
        "            torch.nn.Linear(10, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x, (hidden, cell) = self.rnn(x)\n",
        "        x = self.layers(x[:, -1, ])\n",
        "        return x\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = Net(data_dim, hidden_dim, output_dim, 4).to(device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joxB0eJC6xyu"
      },
      "source": [
        "# train-test dataset to input\n",
        "\n",
        "trainX, trainY = make_sequence(train_set, seq_length)\n",
        "testX, testY = make_sequence(test_set, seq_length)\n",
        "\n",
        "# convert to tensor\n",
        "trainX_tensor = torch.FloatTensor(trainX).to(device)\n",
        "trainY_tensor = torch.FloatTensor(trainY).to(device)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX).to(device)\n",
        "testY_tensor = torch.FloatTensor(testY).to(device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY1eq0rUuRkY"
      },
      "source": [
        "!pip3 install adamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGPE5NAUW_FK"
      },
      "source": [
        "from adamp import AdamP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "2CGHQdJWuU26",
        "outputId": "177b87e8-b218-4ca7-d916-04a2c9c5768e"
      },
      "source": [
        "'''from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "folds=[]\n",
        "for train_idx, valid_idx in kf.split(trainX):\n",
        "    folds.append((train_idx, valid_idx))    \n",
        "\n",
        "for fold in range(3):\n",
        "    best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\n",
        "    train_idx = folds[fold][0]\n",
        "    valid_idx = folds[fold][1]\n",
        "\n",
        "    # cuda cache 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    #train fold, validation fold 분할\n",
        "    train_answer = trainY.iloc[train_idx]\n",
        "    test_answer  = trainY.iloc[valid_idx]\n",
        "\n",
        "    train_df = trainX.iloc[train_idx]\n",
        "    test_df = trainX.iloc[valid_idx]\n",
        "    break'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4f123774e5d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#train fold, validation fold 분할\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_answer\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoapouYm6xyv"
      },
      "source": [
        "# loss & optimizer setting\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = AdamP(net.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "# start training\n",
        "net.train()\n",
        "best_loss = int(1e9)\n",
        "\n",
        "for i in range(iterations):\n",
        "    outputs = net(trainX_tensor)\n",
        "    loss = criterion(outputs, trainY_tensor)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i%100 == 0:\n",
        "        print('Epoch {}, Loss {:.5f}'.format(i, loss.item()))\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_model = net\n",
        "    scheduler.step()\n",
        "\n",
        "best_model.eval()\n",
        "dangjin_floating_pred = []\n",
        "for i in range(672):\n",
        "    x_input  = df[-672-24+i:-672+i].reshape(1,24,6)\n",
        "\n",
        "    x_input = torch.Tensor(x_input)\n",
        "    x_input = x_input.to(device)\n",
        "    predict = best_model(x_input).cpu().detach().numpy()\n",
        "\n",
        "    dangjin_floating_pred.append(predict[0][0])\n",
        "    new_input = predict.reshape((1))\n",
        "    df[-672-24+i:-672+i][-1][-1] = new_input[0]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vDDmlnPZv36",
        "outputId": "d26c25d7-ff59-45e9-ea34-cb48a118f968"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  downcast=downcast,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E77y1e7vFeui",
        "outputId": "9c193c18-7768-4478-987a-66e888bf99cf"
      },
      "source": [
        "df = train_datast(energy, dangjin_interpolated, target='dangjin_warehouse')\n",
        "\n",
        "train_size = int(len(df) * 1)\n",
        "train_set = df[:train_size]\n",
        "test_set = df[train_size-672:]\n",
        "train_set.shape, test_set.shape\n",
        "\n",
        "\n",
        "# train-test dataset to input\n",
        "\n",
        "trainX, trainY = make_sequence(train_set, seq_length)\n",
        "testX, testY = make_sequence(test_set, seq_length)\n",
        "\n",
        "# convert to tensor\n",
        "trainX_tensor = torch.FloatTensor(trainX).to(device)\n",
        "trainY_tensor = torch.FloatTensor(trainY).to(device)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX).to(device)\n",
        "testY_tensor = torch.FloatTensor(testY).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26280, 6), (672, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1fE9gbyF6q5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LJyJpuzGCG6",
        "outputId": "968301e5-d938-415b-d6a3-f48176dae0f9"
      },
      "source": [
        "# loss & optimizer setting\n",
        "net = Net(data_dim, hidden_dim, output_dim, 4).to(device)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# start training\n",
        "net.train()\n",
        "best_loss = int(1e9)\n",
        "\n",
        "for i in range(iterations):\n",
        "    outputs = net(trainX_tensor)\n",
        "    loss = criterion(outputs, trainY_tensor)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i%100 == 0:\n",
        "        print('Epoch {}, Loss {:.5f}'.format(i, loss.item()))\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_model = net\n",
        "\n",
        "best_model.eval()\n",
        "dangjin_warehouse_pred = []\n",
        "for i in range(672):\n",
        "    x_input  = df[-672-24+i:-672+i].reshape(1,24,6)\n",
        "\n",
        "    x_input = torch.Tensor(x_input)\n",
        "    x_input = x_input.to(device)\n",
        "    predict = best_model(x_input).cpu().detach().numpy()\n",
        "\n",
        "    dangjin_warehouse_pred.append(predict[0][0])\n",
        "    new_input = predict.reshape((1))\n",
        "    df[-672-24+i:-672+i][-1][-1] = new_input[0]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss 29104.04688\n",
            "Epoch 100, Loss 21044.23633\n",
            "Epoch 200, Loss 21043.84180\n",
            "Epoch 300, Loss 21043.81641\n",
            "Epoch 400, Loss 2698.15503\n",
            "Epoch 500, Loss 1220.55444\n",
            "Epoch 600, Loss 1014.64471\n",
            "Epoch 700, Loss 891.23059\n",
            "Epoch 800, Loss 840.61072\n",
            "Epoch 900, Loss 792.05908\n",
            "Epoch 1000, Loss 714.65308\n",
            "Epoch 1100, Loss 624.62286\n",
            "Epoch 1200, Loss 561.84656\n",
            "Epoch 1300, Loss 511.23148\n",
            "Epoch 1400, Loss 481.55777\n",
            "Epoch 1500, Loss 508.91702\n",
            "Epoch 1600, Loss 504.68076\n",
            "Epoch 1700, Loss 428.37595\n",
            "Epoch 1800, Loss 352.46747\n",
            "Epoch 1900, Loss 286.81018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRrZdGy6GGch"
      },
      "source": [
        "df = train_datast(energy, dangjin_interpolated, target='dangjin')\n",
        "\n",
        "train_size = int(len(df) * 1)\n",
        "train_set = df[:train_size]\n",
        "test_set = df[train_size-672:]\n",
        "train_set.shape, test_set.shape\n",
        "\n",
        "\n",
        "# train-test dataset to input\n",
        "\n",
        "trainX, trainY = make_sequence(train_set, seq_length)\n",
        "testX, testY = make_sequence(test_set, seq_length)\n",
        "\n",
        "# convert to tensor\n",
        "trainX_tensor = torch.FloatTensor(trainX).to(device)\n",
        "trainY_tensor = torch.FloatTensor(trainY).to(device)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX).to(device)\n",
        "testY_tensor = torch.FloatTensor(testY).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jF-dJ1DGqKQ"
      },
      "source": [
        "# loss & optimizer setting\n",
        "net = Net(data_dim, hidden_dim, output_dim, 4).to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# start training\n",
        "net.train()\n",
        "best_loss = int(1e9)\n",
        "\n",
        "for i in range(iterations):\n",
        "    outputs = net(trainX_tensor)\n",
        "    loss = criterion(outputs, trainY_tensor)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i%100 == 0:\n",
        "        print('Epoch {}, Loss {:.5f}'.format(i, loss.item()))\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_model = net\n",
        "\n",
        "best_model.eval()\n",
        "dangjin_pred = []\n",
        "for i in range(672):\n",
        "    x_input  = df[-672-24+i:-672+i].reshape(1,24,6)\n",
        "\n",
        "    x_input = torch.Tensor(x_input)\n",
        "    x_input = x_input.to(device)\n",
        "    predict = best_model(x_input).cpu().detach().numpy()\n",
        "\n",
        "    dangjin_pred.append(predict[0][0])\n",
        "    new_input = predict.reshape((1))\n",
        "    df[-672-24+i:-672+i][-1][-1] = new_input[0]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBevoC4fH3DE"
      },
      "source": [
        "## 울산\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIihJ__oHylC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a82ae6-2b68-4f37-d8c4-6886838b9cac"
      },
      "source": [
        "df = train_datast(energy, ulsan_interpolated, target='ulsan')\n",
        "\n",
        "train_size = int(len(df) * 1)\n",
        "train_set = df[:train_size]\n",
        "test_set = df[train_size-672:]\n",
        "train_set.shape, test_set.shape\n",
        "\n",
        "\n",
        "# train-test dataset to input\n",
        "\n",
        "trainX, trainY = make_sequence(train_set, seq_length)\n",
        "testX, testY = make_sequence(test_set, seq_length)\n",
        "\n",
        "# convert to tensor\n",
        "trainX_tensor = torch.FloatTensor(trainX).to(device)\n",
        "trainY_tensor = torch.FloatTensor(trainY).to(device)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX).to(device)\n",
        "testY_tensor = torch.FloatTensor(testY).to(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  downcast=downcast,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je7LsRkZH2kD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82e688a-14d3-4d3d-82aa-ebe80f3d828c"
      },
      "source": [
        "# loss & optimizer setting\n",
        "net = Net(data_dim, hidden_dim, output_dim, 4).to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# start training\n",
        "net.train()\n",
        "best_loss = int(1e9)\n",
        "\n",
        "for i in range(iterations):\n",
        "    outputs = net(trainX_tensor)\n",
        "    loss = criterion(outputs, trainY_tensor)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i%100 == 0:\n",
        "        print('Epoch {}, Loss {:.5f}'.format(i, loss.item()))\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        best_model = net\n",
        "\n",
        "best_model.eval()\n",
        "ulsan_pred = []\n",
        "for i in range(672):\n",
        "    x_input  = df[-672-24+i:-672+i].reshape(1,24,6)\n",
        "\n",
        "    x_input = torch.Tensor(x_input)\n",
        "    x_input = x_input.to(device)\n",
        "    predict = best_model(x_input).cpu().detach().numpy()\n",
        "\n",
        "    ulsan_pred.append(predict[0][0])\n",
        "    new_input = predict.reshape((1))\n",
        "    df[-672-24+i:-672+i][-1][-1] = new_input[0]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss 13939.99902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhUThFrGWKb0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}