{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xdkWrmLduoW"
   },
   "source": [
    "pytorch기반 간단한 lstm 모델입니다. 중간중간 불필요한 코드는 주석 처리 하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt1wBHnHduoZ"
   },
   "source": [
    "## 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T13:24:30.048130Z",
     "start_time": "2022-02-21T13:24:27.195521Z"
    },
    "id": "uXia0OHkkSDo"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMA-gTz7kSDr"
   },
   "source": [
    "### 예측해야될 데이터\n",
    "---\n",
    "- dangjin_floating : 당진수상태양광 발전량(KW)\n",
    "- dangjin_warehouse : 당진자재창고태양광 발전량(KW)\n",
    "- dangjin : 당진태양광 발전량(KW)\n",
    "- ulsan : 울산태양광 발전량(KW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRcHp2rCeLim"
   },
   "outputs": [],
   "source": [
    "def Get_Preprocessing_Data(df, column):\n",
    "    \"\"\"\n",
    "    모델에 입력될 데이터를 전처리하여 float형태로 변환한 후 반환하는 함수\n",
    "\n",
    "    파라미터\n",
    "    ---\n",
    "    df : DataFrame\n",
    "        energy 데이터\n",
    "    column : str\n",
    "        현재 예측할 발전소 이름\n",
    "\n",
    "    returns \n",
    "    ---\n",
    "    df : DataFrame\n",
    "        전처리가 적용된 DataFrame\n",
    "    \"\"\"\n",
    "    df = df[['time', column]]\n",
    "    # 결측치는 평균 값으로 처리\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    # time 컬럼을 index로 지정\n",
    "    df = df.set_index('time')\n",
    "    df = df[column].values.astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T13:25:00.016291Z",
     "start_time": "2022-02-21T13:25:00.011289Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02ovCwcUduog",
    "outputId": "a8e5bee5-67d5-47d6-9853-27fd47ec171f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n"
     ]
    }
   ],
   "source": [
    "energy = pd.read_csv('energy.csv')  # 발전소별 발전량\n",
    "\n",
    "# 전처리 함수 적용\n",
    "ulsan = Get_Preprocessing_Data(energy, 'ulsan')\n",
    "dangjin_floating = Get_Preprocessing_Data(energy, 'dangjin_floating')\n",
    "dangjin_warehouse = Get_Preprocessing_Data(energy, 'dangjin_warehouse')\n",
    "dangjin = Get_Preprocessing_Data(energy, 'dangjin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T13:25:32.432605Z",
     "start_time": "2022-02-21T13:25:32.418643Z"
    },
    "id": "jszcrSxhduoh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def make_batch(input_data, sl=24):\n",
    "    \"\"\"\n",
    "    energy 데이터를 sequence length길이에 맞춰 input형태로 변환 시 켜준다.\n",
    "    그리고 train데이터 셋과 test데이터셋을 나눈다.\n",
    "\n",
    "    파라미터 \n",
    "    ---\n",
    "    input_data : \n",
    "        energy 데이터\n",
    "    sl : int\n",
    "        sequence length \n",
    "\n",
    "    returns\n",
    "    train_x : Tensor\n",
    "        model의 학습용 input data\n",
    "    train_y : Tensor\n",
    "        model의 학습용 target data    \n",
    "    valid_x : Tensor\n",
    "        model의 검증용 input data\n",
    "    valid_y : Tensor\n",
    "        model의 검증용 target data    \n",
    "    \"\"\"\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-sl):\n",
    "        train_seq = input_data[i:i+sl]\n",
    "        train_label = input_data[i+sl:i+sl+1]\n",
    "        train_x.append(train_seq)\n",
    "        train_y.append(train_label)\n",
    "\n",
    "    tensor_x, tensor_y = torch.Tensor(train_x), torch.Tensor(train_y)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        tensor_x, tensor_y, test_size=0.3)\n",
    "\n",
    "    return train_x, valid_x, train_y, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vI900CFQjW3U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    mini batch 학습을 위한 customdataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tensor_x, tensor_y):\n",
    "        self.x_data = tensor_x\n",
    "        self.y_data = tensor_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T13:25:50.339499Z",
     "start_time": "2022-02-21T13:25:50.321548Z"
    },
    "id": "yznYWHlYduoh"
   },
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "class simple_lstm(nn.Module):\n",
    "\n",
    "    def __init__(self, input_vector, sl, output_vector, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_vector = input_vector\n",
    "        self.sequence_length = sl\n",
    "        self.output_vector = output_vector\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_vector, hidden_size=self.output_vector,\n",
    "                            num_layers=self.num_layers, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.output_vector, 50),\n",
    "            nn.Linear(50, 30),\n",
    "            nn.Linear(30, 10),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)  # (hidden, cell) 데이터는 사용하지 않음\n",
    "        return self.linear(output[:, -1, :])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_oe4kcziJSX"
   },
   "outputs": [],
   "source": [
    "def training(model, EPOCHS, optimizer, criterion, train_loader, valid_loader):\n",
    "    \"\"\"\n",
    "    model을 학습하는함수 \n",
    "    검증데이터셋을 이용하여 가장 성능이 좋은 모델을 반환한다.\n",
    "    \"\"\"\n",
    "    best_model = model\n",
    "    BEST_LOSS = int(1e9)\n",
    "    for i in range(EPOCHS):\n",
    "\n",
    "        TRAIN_LOSS = 0\n",
    "        VALID_LOSS = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            model.train()\n",
    "            tensor_x, tensor_y = batch\n",
    "            # optimizer 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tensor_x = tensor_x.to(device)\n",
    "            tensor_y = tensor_y.to(device)\n",
    "            output = model(tensor_x)\n",
    "            loss = criterion(output, tensor_y.view(-1, 1))\n",
    "\n",
    "            # loss 누적\n",
    "            TRAIN_LOSS += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch_idx, batch in enumerate(valid_loader):\n",
    "            tensor_x, tensor_y = batch\n",
    "            with torch.no_grad():\n",
    "                tensor_x = tensor_x.to(device)\n",
    "                tensor_y = tensor_y.to(device)\n",
    "                output = model(tensor_x)\n",
    "                loss = criterion(output, tensor_y.view(-1, 1))\n",
    "                VALID_LOSS += loss.item()\n",
    "\n",
    "        # best 모델 저장\n",
    "        if VALID_LOSS < BEST_LOSS:\n",
    "            best_model = model\n",
    "            BEST_LOSS = VALID_LOSS\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch {}, train_Loss {:.5f}, valid_Loss {:.5f}'.format(\n",
    "                i, TRAIN_LOSS, VALID_LOSS))\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcEjUJDPr2xN"
   },
   "outputs": [],
   "source": [
    "def inference(df, model):\n",
    "    \"\"\"\n",
    "    학습된 모델을 이용하여 발전소별 발전량을 예측하는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    x_input = np.array(df[-48:])  # next value based on data of last year\n",
    "    x_input = x_input.reshape((1, 48, 1))\n",
    "    model_pred = []\n",
    "\n",
    "    for i in range(672):\n",
    "\n",
    "        x_input = torch.Tensor(x_input)\n",
    "        x_input = x_input.to(device)\n",
    "        predict = model(x_input).cpu().detach().numpy()\n",
    "\n",
    "        new_input = predict.reshape((1, 1, 1))\n",
    "        x_input = np.concatenate((x_input[:, -47:].cpu(), new_input), axis=1)\n",
    "        model_pred.append(predict[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6LknmMPduoi"
   },
   "source": [
    "### dangjin_floating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrhgXcg6f16T"
   },
   "outputs": [],
   "source": [
    "def train_and_inference(df):\n",
    "    \"\"\"\n",
    "    각 발전소 별 모델 학습하고 \n",
    "    성능이 가장 좋은 모델을 이용하여 추론한 결과를 반환하는 함수\n",
    "\n",
    "    파라미터 \n",
    "    ---\n",
    "    df : dataframe\n",
    "        발전소 별 발전량\n",
    "\n",
    "    return\n",
    "    ---\n",
    "    pred : list\n",
    "        학습된 모델을 이용한 예측값\n",
    "    \"\"\"\n",
    "\n",
    "    # hyper parameters\n",
    "    SEQUENCE_LENGTH = 48\n",
    "    INPUT_VECTOR = 1\n",
    "    OUTPUT_VECTOR = 100\n",
    "    NUM_LAYERS = 4\n",
    "    EPOCHS = 2000\n",
    "    LR = 0.0001\n",
    "\n",
    "    # 모델 선언\n",
    "    lstm = simple_lstm(INPUT_VECTOR, SEQUENCE_LENGTH,\n",
    "                       OUTPUT_VECTOR, NUM_LAYERS).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=LR)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y = make_batch(\n",
    "        df.reshape(-1, 1), SEQUENCE_LENGTH)\n",
    "    trn_data = CustomDataset(train_x, train_y)\n",
    "    val_data = CustomDataset(valid_x, valid_y)\n",
    "    train_loader = DataLoader(trn_data, batch_size=256, shuffle=True)\n",
    "    valid_loader = DataLoader(val_data, batch_size=256, shuffle=True)\n",
    "\n",
    "    best_model = training(lstm, EPOCHS, optimizer,\n",
    "                          criterion, train_loader, valid_loader)\n",
    "    pred = inference(df, best_model)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5v53pMujtMHU",
    "outputId": "979ac5d2-77c4-4b5b-9b93-42c868f16336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_Loss 3628771.52344, valid_Loss 1528781.82031\n",
      "Epoch 100, train_Loss 111320.26965, valid_Loss 52839.59729\n",
      "Epoch 200, train_Loss 101892.66162, valid_Loss 52973.18640\n",
      "Epoch 300, train_Loss 86359.60925, valid_Loss 55953.62732\n",
      "Epoch 400, train_Loss 61679.72440, valid_Loss 62537.70178\n",
      "Epoch 500, train_Loss 35888.50278, valid_Loss 70154.57422\n",
      "Epoch 600, train_Loss 19431.43362, valid_Loss 76953.08704\n",
      "Epoch 700, train_Loss 11116.01704, valid_Loss 81847.70544\n",
      "Epoch 800, train_Loss 10459.42016, valid_Loss 85358.49255\n",
      "Epoch 900, train_Loss 4990.33447, valid_Loss 84845.09924\n",
      "Epoch 1000, train_Loss 1897.68170, valid_Loss 83899.68140\n",
      "Epoch 1100, train_Loss 1969.11242, valid_Loss 83475.45581\n",
      "Epoch 1200, train_Loss 1365.27160, valid_Loss 81939.67615\n",
      "Epoch 1300, train_Loss 2567.59942, valid_Loss 82334.48157\n",
      "Epoch 1400, train_Loss 1515.97293, valid_Loss 80986.61487\n",
      "Epoch 1500, train_Loss 425.90983, valid_Loss 79563.20227\n",
      "Epoch 1600, train_Loss 842.40405, valid_Loss 79141.02783\n",
      "Epoch 1700, train_Loss 992.18224, valid_Loss 79311.46545\n",
      "Epoch 1800, train_Loss 238.45241, valid_Loss 78584.02417\n",
      "Epoch 1900, train_Loss 450.26929, valid_Loss 77764.48071\n",
      "Epoch 0, train_Loss 1005106.79883, valid_Loss 416616.68164\n",
      "Epoch 100, train_Loss 31279.05469, valid_Loss 12915.21133\n",
      "Epoch 200, train_Loss 28646.63098, valid_Loss 13155.35770\n",
      "Epoch 300, train_Loss 25125.99226, valid_Loss 13910.81082\n",
      "Epoch 400, train_Loss 19387.16003, valid_Loss 15343.72079\n",
      "Epoch 500, train_Loss 11961.43603, valid_Loss 17343.99744\n",
      "Epoch 600, train_Loss 6177.33817, valid_Loss 20506.50668\n",
      "Epoch 700, train_Loss 3244.55376, valid_Loss 22211.87646\n",
      "Epoch 800, train_Loss 1775.25180, valid_Loss 23075.23431\n",
      "Epoch 900, train_Loss 1209.73351, valid_Loss 22934.01285\n",
      "Epoch 1000, train_Loss 954.15351, valid_Loss 22775.05948\n",
      "Epoch 1100, train_Loss 1019.33236, valid_Loss 22496.73355\n",
      "Epoch 1200, train_Loss 847.89599, valid_Loss 22092.29413\n",
      "Epoch 1300, train_Loss 574.58264, valid_Loss 21773.70898\n",
      "Epoch 1400, train_Loss 523.32856, valid_Loss 21581.16330\n",
      "Epoch 1500, train_Loss 764.79897, valid_Loss 21263.64746\n",
      "Epoch 1600, train_Loss 569.03935, valid_Loss 21107.61978\n",
      "Epoch 1700, train_Loss 494.29050, valid_Loss 20890.21664\n",
      "Epoch 1800, train_Loss 458.95496, valid_Loss 20772.46243\n",
      "Epoch 1900, train_Loss 390.65659, valid_Loss 20473.50095\n",
      "Epoch 0, train_Loss 2088289.97266, valid_Loss 893821.43945\n",
      "Epoch 100, train_Loss 65586.69534, valid_Loss 29459.85016\n",
      "Epoch 200, train_Loss 60439.49121, valid_Loss 29609.68573\n",
      "Epoch 300, train_Loss 52651.70160, valid_Loss 31179.55188\n",
      "Epoch 400, train_Loss 37186.02597, valid_Loss 34685.96912\n",
      "Epoch 500, train_Loss 22790.63889, valid_Loss 38377.18701\n",
      "Epoch 600, train_Loss 12343.91899, valid_Loss 43024.43231\n",
      "Epoch 700, train_Loss 5348.10437, valid_Loss 45157.98376\n",
      "Epoch 800, train_Loss 3682.63791, valid_Loss 46468.00232\n",
      "Epoch 900, train_Loss 2725.66031, valid_Loss 46625.90833\n",
      "Epoch 1000, train_Loss 1414.07289, valid_Loss 46846.26910\n",
      "Epoch 1100, train_Loss 799.96452, valid_Loss 46008.67102\n",
      "Epoch 1200, train_Loss 765.68150, valid_Loss 46479.47302\n",
      "Epoch 1300, train_Loss 582.93599, valid_Loss 45931.13873\n",
      "Epoch 1400, train_Loss 310.87945, valid_Loss 45249.05139\n",
      "Epoch 1500, train_Loss 4865.83462, valid_Loss 44149.07422\n",
      "Epoch 1600, train_Loss 325.12871, valid_Loss 44330.88934\n",
      "Epoch 1700, train_Loss 568.87176, valid_Loss 44243.24731\n",
      "Epoch 1800, train_Loss 130.79271, valid_Loss 43757.51752\n",
      "Epoch 1900, train_Loss 747.61654, valid_Loss 43619.24957\n",
      "Epoch 0, train_Loss 4770470.38672, valid_Loss 2013588.47656\n",
      "Epoch 100, train_Loss 148796.72363, valid_Loss 68089.21472\n",
      "Epoch 200, train_Loss 133581.07825, valid_Loss 69471.71021\n",
      "Epoch 300, train_Loss 105589.31348, valid_Loss 76233.45105\n",
      "Epoch 400, train_Loss 65219.68271, valid_Loss 88161.46863\n",
      "Epoch 500, train_Loss 36469.18530, valid_Loss 98745.04858\n",
      "Epoch 600, train_Loss 14906.17744, valid_Loss 105444.46704\n",
      "Epoch 700, train_Loss 9411.28585, valid_Loss 106464.20142\n",
      "Epoch 800, train_Loss 5377.77896, valid_Loss 108290.92627\n",
      "Epoch 900, train_Loss 6818.89256, valid_Loss 107366.61572\n",
      "Epoch 1000, train_Loss 1425.09483, valid_Loss 106320.29248\n",
      "Epoch 1100, train_Loss 1836.53175, valid_Loss 104454.54102\n",
      "Epoch 1200, train_Loss 576.27051, valid_Loss 103733.07349\n",
      "Epoch 1300, train_Loss 398.07480, valid_Loss 103466.47998\n",
      "Epoch 1400, train_Loss 470.30622, valid_Loss 102643.39038\n",
      "Epoch 1500, train_Loss 764.14374, valid_Loss 102067.52539\n",
      "Epoch 1600, train_Loss 1442.56585, valid_Loss 101385.52930\n",
      "Epoch 1700, train_Loss 701.83200, valid_Loss 100849.31824\n",
      "Epoch 1800, train_Loss 286.12932, valid_Loss 100129.87634\n",
      "Epoch 1900, train_Loss 288.64939, valid_Loss 99498.37549\n"
     ]
    }
   ],
   "source": [
    "dangjin_floating_pred = train_and_inference(dangjin_floating)\n",
    "ulsan_pred = train_and_inference(ulsan)\n",
    "dangjin_warehouse_pred = train_and_inference(dangjin_warehouse)\n",
    "dangjin_pred = train_and_inference(dangjin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pl__166duoi"
   },
   "source": [
    "### 정답 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoBygHf5_k2U"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CslHT61Vu4h3"
   },
   "outputs": [],
   "source": [
    "submission.iloc[:24*28, 1] = dangjin_floating_pred\n",
    "submission.iloc[:24*28, 2] = dangjin_warehouse_pred\n",
    "submission.iloc[:24*28, 3] = dangjin_pred\n",
    "submission.iloc[:24*28, 4] = ulsan_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJmfz90svFfP"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xM30vnrxduol"
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "동서발전_단일변수_lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
